## 4.2 离散信源及其信息测度

### 4.2.1 离散信源信息的度量

某事件出现所提供的信息量，是该事件出现概率的函数。
* 离散信源 $X$ 的统计特性（概率场）可描述为： 
    $$
    \begin{matrix}
    X: & x_1 & x_2 & \cdot\cdot\cdot & x_N \\
    p(X): & p(x_1) & p(x_2) & \cdot\cdot\cdot & p(x_N)
    \end{matrix}
    $$
	且满足 $\sum_{i=1}^{N} p(x_i) = 1$。
通信系统可简化为：信源 $\rightarrow$ 信道 $\rightarrow$ 信宿。
信宿获得的信息量 $I(x_i)$ 应是该符号出现概率 $p(x_i)$ 的某种函数：$I(x_i) = f[p(x_i)]$
* **信息量函数应满足的要素：**
    1.  **概率的函数:** $I(x_i) = f[p(x_i)]$。
    2.  **可加性:** 若符号 $x_i, x_j$ 独立，则 $I(x_i x_j) = f[p(x_i)p(x_j)] = f[p(x_i)] + f[p(x_j)]$。
    3.  **反比关系:** 概率越小，信息量越大。
* 对数函数 $f[p(x_i)] = \log(\frac{1}{p(x_i)})$ 可以同时满足这三个要素。
#### 定义 4.2.1 (信息量)
符号出现概率的倒数的对数定义为该符号的**信息量**（自信息）。
$$
I(x_i) = \log\left(\frac{1}{p(x_i)}\right) = -\log p(x_i)
$$

* **信息量的单位** (与对数的底有关):
    * 底为 2: **比特 (bit)** (本书默认单位)
    * 底为 e: **奈特 (nat)**
    * 底为 10: **哈特 (hart)**
#### 【例4.2.1】
* **问题:** 某离散信源由 $\{0, 1, 2, 3\}$ 组成，概率场为：
    $p(0) = 3/8$, $p(1) = 1/4$, $p(2) = 1/4$, $p(3) = 1/8$。
    假定符号独立，求符号序列 S = "113200" 的信息量。
* **解:**
    -   按定义，信息量具有可加性（因符号独立）：
        $I(S) = I(1) + I(1) + I(3) + I(2) + I(0) + I(0)$
    -   计算每个符号的信息量 (以 2 为底):
        -   $I(1) = \log_2(1 / (1/4)) = \log_2 4 = 2$ bit
        -   $I(2) = \log_2(1 / (1/4)) = \log_2 4 = 2$ bit
        -   $I(3) = \log_2(1 / (1/8)) = \log_2 8 = 3$ bit
        -   $I(0) = \log_2(1 / (3/8)) = \log_2(8/3) \approx 1.415$ bit
    -   计算总信息量:
        $I(S) = 2 + 2 + 3 + 2 + 1.415 + 1.415 = 11.83$ (bit)

### 4.2.2 离散信源的平均信息量——信源的熵

信源的总体信息测度可用信源符号集中每个符号的**平均信息量**来描述。
#### 定义 4.2.2 (信源的熵)
离散信源 $X$ 的**熵** $H(X)$ 定义为信源中每个符号所含的统计平均信息量：
$$
H(X) = E[I(x_i)] = \sum_{i=1}^{N} p(x_i) I(x_i) = -\sum_{i=1}^{N} p(x_i) \log p(x_i)
$$
* 单位: 比特/符号。
#### 【例4.2.2】
* **问题:** 计算例4.2.1中信源的熵。
* **解:**
    -   $H(X) = -\sum_{i=1}^{4} p(x_i) \log p(x_i)$
    -   $H(X) = -\left[ \frac{3}{8}\log\frac{3}{8} + \frac{1}{4}\log\frac{1}{4} + \frac{1}{4}\log\frac{1}{4} + \frac{1}{8}\log\frac{1}{8} \right]$
    -   $H(X) = -\left[ \frac{3}{8}(-1.415) + \frac{1}{4}(-2) + \frac{1}{4}(-2) + \frac{1}{8}(-3) \right]$
    -   $H(X) = -[-0.5306 - 0.5 - 0.5 - 0.375] = 1.906$ (比特/符号)
#### 【例4.2.3】
* **问题:** 信源同上。消息 (序列略) 共包含57个符号，其中 "0" 出现23次, "1" 出现14次, "2" 出现13次, "3" 出现7次。
    (1) 求该消息的总信息量。
    (2) 利用信源的熵估算该消息的信息量。
* **解 (1):**
    -   总信息量 $I = \sum n_i I(x_i) = -\sum_{i=1}^{4} n_i \log p(x_i)$
    -   $I = -\left[ 23 \log\frac{3}{8} + 14 \log\frac{1}{4} + 13 \log\frac{1}{4} + 7 \log\frac{1}{8} \right]$
    -   $I = -[23(-1.415) + 14(-2) + 13(-2) + 7(-3)] = 107.55$ (bit)
* **解 (2):**
    -   估算信息量 $I \approx (\text{总符号数}) \times H(X)$
    -   $I \approx (23+14+13+7) \times 1.906 = 57 \times 1.906 = 108.62$ (bit)
* **结论: 随着消息中符号数的增加，(1) 和 (2) 的结果趋于一致。**

### 4.2.3 熵的最大化

#### 定义 4.2.3 (凸集)
设 $\{x\}$ 是n维实向量集合。若对任意 $x_i, x_j \in \{x\}$ 和任意实数 $0 \le \alpha \le 1$，均有：$\alpha x_i + (1-\alpha)x_j \in \{x\}$，则称集合 $\{x\}$ 是**凸集**。
#### 定义 4.2.4 (凸函数)
设函数 $f(x)$ 定义在凸集 $\{x\}$ 上，若对任意 $x_i, x_j \in \{x\}$ 和 $0 \le \alpha \le 1$：
* **$\cup$形凸函数 (下凸):** $f(\alpha x_i + (1-\alpha)x_j) \le \alpha f(x_i) + (1-\alpha)f(x_j)$
* **$\cap$形凸函数 (上凸):** $f(\alpha x_i + (1-\alpha)x_j) \ge \alpha f(x_i) + (1-\alpha)f(x_j)$
* **$\cap$形凸函数的不等式 (Jensen不等式 推广):**
    若 $f(x)$ 为 $\cap$形凸函数， $p=(p_1, \dots, p_N)$ 是一组概率，则有：
    $$
    f\left(\sum_{i=1}^{N} p_i x_i\right) \ge \sum_{i=1}^{N} p_i f(x_i)
    $$
    * **证明 (数学归纳法):**
        1.  **N=2 (n=1) 时:** $f(p_1 x_1 + p_2 x_2) \ge p_1 f(x_1) + p_2 f(x_2)$。
            令 $\alpha = p_1$，则 $p_2 = 1 - \alpha$。
            $f(\alpha x_1 + (1-\alpha) x_2) \ge \alpha f(x_1) + (1-\alpha) f(x_2)$。
            根据 $\cap$形凸函数定义，(1) 成立。
        2.  **假设 N=k+1 (n=k) 时成立:** $f(\sum_{i=1}^{k+1} p_i x_i) \ge \sum_{i=1}^{k+1} p_i f(x_i)$。
        3.  **证明 N=k+2 (n=k+1) 时成立:**
            -   $f(\sum_{i=1}^{k+2} p_i x_i) = f\left(p_1 x_1 + (1-p_1) \sum_{i=2}^{k+2} \frac{p_i}{1-p_1} x_i\right)$
            -   应用 (1) 的结论 (令 $\alpha=p_1$, $x_i \to x_1$, $x_j \to \sum_{i=2}^{k+2} \frac{p_i}{1-p_1} x_i$):
                $\ge p_1 f(x_1) + (1-p_1) f\left(\sum_{i=2}^{k+2} \frac{p_i}{1-p_1} x_i\right)$
            -   令 $p_i' = \frac{p_i}{1-p_1}$ ($i=2, \dots, k+2$)。 $\sum_{i=2}^{k+2} p_i' = 1$，这是一组 $k+1$ 项的概率。
            -   应用 (2) 的假设: $f(\sum_{i=2}^{k+2} p_i' x_i) \ge \sum_{i=2}^{k+2} p_i' f(x_i)$
            -   代回:
                $\ge p_1 f(x_1) + (1-p_1) \sum_{i=2}^{k+2} \frac{p_i}{1-p_1} f(x_i)$
                $= p_1 f(x_1) + \sum_{i=2}^{k+2} p_i f(x_i) = \sum_{i=1}^{k+2} p_i f(x_i)$
            -   证毕。

* **重要对数关系式 (Gibbs' inequality):**
    * 对数函数 $\log x$ (底数 > 1, $x > 0$) 是一个 $\cap$形凸函数。
    * 设有两组不同的概率 $p=(p_1, \dots, p_N)$ 和 $p'=(p_1', \dots, p_N')$。
    * 由 $\cap$形凸函数不等式 (令 $x_i = p_i' / p_i$):
        $$
        \log\left(\sum_{i=1}^{N} p_i \frac{p_i'}{p_i}\right) \ge \sum_{i=1}^{N} p_i \log\left(\frac{p_i'}{p_i}\right)
        $$
    * 左侧 = $\log(\sum p_i') = \log 1 = 0$。
    * $0 \ge \sum p_i (\log p_i' - \log p_i) = \sum p_i \log p_i' - \sum p_i \log p_i$
    * (原文 (4.2.12) 写为 $-\sum p_i \log p_i' \ge -\sum p_i \log p_i$, 并在后续 (4.2.18) 中使用了这个形式)
        $$
        -\sum_{i=1}^{N} p_i \log p_i' \ge -\sum_{i=1}^{N} p_i \log p_i
        $$
        (注: $H(X) \le -\sum p_i \log p_i')$
#### 定理 4.2.1
熵函数 $H(X)$ 是概率 $(p(x_1), \dots, p(x_N))$ 的 $\cap$形凸函数。
* **证明:**
    -   需证明: $H(\alpha p + (1-\alpha)p') \ge \alpha H(p) + (1-\alpha)H(p')$
    -   令 $\omega = \alpha p + (1-\alpha)p'$，即 $\omega_i = \alpha p(x_i) + (1-\alpha)p(x_i')$。 $\omega$ 也是一组概率。
    -   $H(\omega) = -\sum \omega_i \log \omega_i$
    -   $= -\sum (\alpha p(x_i) + (1-\alpha)p(x_i')) \log \omega_i$
    -   $= -\alpha \sum p(x_i) \log \omega_i - (1-\alpha) \sum p(x_i') \log \omega_i$
    -   应用对数关系式 (4.2.12) (即 $H(p) \le -\sum p_i \log \omega_i$):
        -   $-\sum p(x_i) \log \omega_i \ge -\sum p(x_i) \log p(x_i) = H(p)$
        -   $-\sum p(x_i') \log \omega_i \ge -\sum p(x_i') \log p(x_i') = H(p')$
    -   代回 $H(\omega)$ 的表达式:
        $H(\omega) \ge \alpha H(p) + (1-\alpha) H(p')$
    -   即 $H(\alpha p + (1-\alpha)p') \ge \alpha H(p) + (1-\alpha)H(p')$。 证毕。
#### 定理 4.2.2
当信源符号服从**均匀分布**时，其熵函数 $H(X)$ 达到最大值。
$$
p(x_i) = \frac{1}{N}, \quad i=1, 2, \dots, N
$$
* **证明 (拉格朗日求极值法):**
    -   目标函数: $H(X) = -\sum p(x_i) \log p(x_i)$
    -   约束条件: $\sum p(x_i) = 1$
    -   构造辅助函数:
        $F[p(x_1), \dots] = H(X) + \lambda \left(\sum_{i=1}^{N} p(x_i) - 1\right)$
        $F = -\sum p(x_i) \log p(x_i) + \lambda \left(\sum p(x_i) - 1\right)$
    -   对 $p(x_i)$ 求偏导数并令其为 0:
        (注: 原文 $\frac{d}{dx}(x \log x) = \log x + 1$, 这在 $\log$ 为 $\ln$ 时成立, 或 $\log$ 为 $\log_2$ 且 $1 = \log_2 e$。这里遵循原文的导数)
        $\frac{\partial F}{\partial p(x_i)} = -(\log p(x_i) + 1) + \lambda = 0$
    -   解得: $\log p(x_i) = \lambda - 1$
    -   $p(x_i) = 2^{(\lambda - 1)}$ (假设底为2)
    -   $p(x_i)$ 对所有 $i$ 都是常数。
    -   代入约束条件:
        $\sum_{i=1}^{N} p(x_i) = \sum_{i=1}^{N} 2^{(\lambda - 1)} = N \cdot 2^{(\lambda - 1)} = 1$
    -   $2^{(\lambda - 1)} = 1/N$
    -   $p(x_i) = 1/N$。 证毕。

* **熵的最大值:**
    $$
    \max H(X) = H\left(\frac{1}{N}, \dots, \frac{1}{N}\right) = -\sum_{i=1}^{N} \frac{1}{N} \log \frac{1}{N} = -N \left(\frac{1}{N} \log \frac{1}{N}\right) = -\log\frac{1}{N} = \log N
    $$
* (直观上，等概率分布时，信源的不确定性最大。)
#### 【例4.2.4】
* **问题:** 由 $0, 1, 2, 3$ 四种符号组成的离散信源，各符号等概分布，求信源的熵。
* **解:**
    -   $N = 4$。
    -   $H(X) = \log N = \log 4 = 2$ (比特/符号)。
    -   (此值 2 > 例4.2.2 中的 1.906)
* **启示: 可通过编码使信源符号接近等概，提高传输效率。**

### 4.2.4 离散信源的联合熵与条件熵

#### 定义 4.2.5 (联合熵)
两离散随机变量 $\{XY\}$ 的**联合熵** $H(XY)$ 定义为：
$$
H(XY) = -\sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i y_j) \log p(x_i y_j)
$$

* **统计独立信源:**
    * 若 $X$ 和 $Y$ 统计独立, $p(x_i y_j) = p(x_i) p(y_j)$。
    * $H(XY) = H(X) + H(Y)$
    * **推导:**
        -   $$\begin{aligned}H(XY) &= -\sum_i \sum_j p(x_i) p(y_j) \log [p(x_i) p(y_j)]\\&= -\sum_i \sum_j p(x_i) p(y_j) [\log p(x_i) + \log p(y_j)]\\&= -\sum_i \sum_j p(x_i) p(y_j) \log p(x_i) - \sum_i \sum_j p(x_i) p(y_j) \log p(y_j)\\&= -\left(\sum_j p(y_j)\right) \left(\sum_i p(x_i) \log p(x_i)\right) - \left(\sum_i p(x_i)\right) \left(\sum_j p(y_j) \log p(y_j)\right)\\&= - (1) \cdot (-H(X)) - (1) \cdot (-H(Y))\\&= H(X) + H(Y)\end{aligned}$$
#### 定义 4.2.6 (条件熵)
* 给定 $Y$ 时 $X$ 的**条件熵** $H(X|Y)$ (当已知 $Y$ 时, $X$ 仍然存在的平均不确定性):
    $$
    H(X|Y) = -\sum_{i=1}^{M} \sum_{j=1}^{N} p(Y=y_j)p(x_i |y_j) \log p(x_i|y_j)＝-\sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i y_j) \log p(x_i|y_j)
    $$
* 给定 $X$ 时 $Y$ 的**条件熵** $H(Y|X)$:
    $$
    H(Y|X) = -\sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i y_j) \log p(y_j|x_i)
    $$
* **条件熵的性质:**
    * $H(X|Y) \le H(X)$
    * (等号仅当 $X, Y$ 相互独立时成立。这说明 $Y$ 的出现总是有助于降低 $X$ 的不确定性。)

## 4.3 离散信道及容量

**单符号离散无记忆信道:** 信道输出的每个符号只与当前输入的符号有关。

### 4.3.1 信道的模型

* 输入: $\{X: x_i, i=1,\dots,M\}$
* 输出: $\{Y: y_j, j=1,\dots,N\}$
* **信道转移概率:** $p(y_j|x_i)$
* **转移矩阵 $p(Y|X)$:** (一个 $M \times N$ 矩阵)
    $$
    p(Y|X) = \begin{bmatrix}
    p(y_1|x_1) & p(y_2|x_1) & \cdots & p(y_N|x_1) \\
    p(y_1|x_2) & p(y_2|x_2) & \cdots & p(y_N|x_2) \\
    \vdots & \vdots & \ddots & \vdots \\
    p(y_1|x_M) & p(y_2|x_M) & \cdots & p(y_N|x_M)
    \end{bmatrix}
    $$
* **后验概率:** $p(x_i|y_j)$ (收到 $y_j$ 时, 推测发送的是 $x_i$ 的概率)
* **后验概率矩阵 $p(X|Y)$:** (一个 $N \times M$ 矩阵)
    $$
    p(X|Y) = \begin{bmatrix}
    p(x_1|y_1) & p(x_2|y_1) & \cdots & p(x_M|y_1) \\
    p(x_1|y_2) & p(x_2|y_2) & \cdots & p(x_M|y_2) \\
    \vdots & \vdots & \ddots & \vdots \\
    p(x_1|y_N) & p(x_2|y_N) & \cdots & p(x_M|y_N)
    \end{bmatrix}
    $$
#### 【例4.3.1】
* **问题:** 某二元离散无记忆对称信道 (BSC)，发 "0" 和 "1" 时，正确接收概率均为 0.99，错误接收概率均为 0.01。求转移矩阵。
* **解:**
    -   $p(y_j=0|x_i=0) = 0.99$
    -   $p(y_j=1|x_i=1) = 0.99$
    -   $p(y_j=1|x_i=0) = 0.01$
    -   $p(y_j=0|x_i=1) = 0.01$
    -   转移矩阵 $p(Y|X)$:
        $$
        p(Y|X) = \begin{bmatrix} 0.99 & 0.01 \\ 0.01 & 0.99 \end{bmatrix}
        $$

### 4.3.2 互信息量

* **先验不确定性 (自信息):** $I(x_i) = \log\frac{1}{p(x_i)}$
* **后验不确定性 (后验信息):** (收到 $y_j$ 后, $x_i$ 的不确定性)
    $I(x_i|y_j) = \log\frac{1}{p(x_i|y_j)}$
#### 定义 4.3.1 (互信息量)
收到符号 $y_j$ 后，有关符号 $x_i$ 的不确定性的消除量，称为**互信息量**。
$$
I(x_i; y_j) = I(x_i) - I(x_i|y_j)
$$

* **互信息量的对称性:** $I(x_i; y_j) = I(y_j; x_i)$
    * **推导:**
        -   $I(x_i; y_j) = \log\frac{1}{p(x_i)} - \log\frac{1}{p(x_i|y_j)} = \log\frac{p(x_i|y_j)}{p(x_i)}$
        -   (根据贝叶斯公式 $p(x_i|y_j)p(y_j) = p(x_i y_j) = p(y_j|x_i)p(x_i)$)
        -   $I(x_i; y_j) = \log\frac{p(x_i y_j)}{p(x_i)p(y_j)}$
        -   $I(x_i; y_j) = \log\frac{p(y_j|x_i)}{p(y_j)}$
        -   $I(x_i; y_j) = \log\frac{1}{p(y_j)} - \log\frac{1}{p(y_j|x_i)} = I(y_j) - I(y_j|x_i) = I(y_j; x_i)$

* **互信息量分析 (收到 $y_j$ 对 $x_i$ 的影响):**
    1.  $p(x_i|y_j) = 1$: $I(x_i|y_j) = 0 \implies I(x_i; y_j) = I(x_i)$。($y_j$ 提供了 $x_i$ 的全部信息)
    2.  $p(x_i) < p(x_i|y_j) < 1$: $0 < I(x_i; y_j) < I(x_i)$。($y_j$ 提供了 $x_i$ 的部分信息)
    3.  $p(x_i|y_j) = p(x_i)$: $I(x_i; y_j) = 0$。($X, Y$ 统计独立, $y_j$ 未提供 $x_i$ 的信息)
    4.  $p(x_i|y_j) < p(x_i)$: $I(x_i; y_j) < 0$。($y_j$ 增加了 $x_i$ 的不确定性)
#### 定义 4.3.2 (平均互信息量)
**平均互信息量** $I(X;Y)$ 定义为 $I(x_i; y_j)$ 的统计平均值：
$$
I(X;Y) = \sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i y_j) I(x_i; y_j)
$$
* (物理意义: 信道每传输一个符号, 信宿平均获得的信息量。)
* **非负性:** $I(X;Y) \ge 0$
    * *(单个互信息量 $I(x_i; y_j)$ 可能为负, 但平均互信息量 $I(X;Y)$ 总是非负的。)*
    * **证明:**
        -   $I(X;Y) = \sum_j p(y_j) I(X; y_j)$, 其中 $I(X; y_j) = \sum_i p(x_i|y_j) I(x_i; y_j)$
        -   $I(X; y_j) = \sum_i p(x_i|y_j) \log\frac{p(x_i|y_j)}{p(x_i)}$
        -   $-I(X; y_j) = \sum_i p(x_i|y_j) \log\frac{p(x_i)}{p(x_i|y_j)}$
        -   (利用 $\log x$ 是 $\cap$形函数 (原文 source 273 称为 $\cup$形, 但使用了 $\sum p_k f(a_k) \le f(\sum p_k a_k)$ 的性质))
        -   $-I(X; y_j) \le \log \left( \sum_i p(x_i|y_j) \frac{p(x_i)}{p(x_i|y_j)} \right) = \log \left( \sum_i p(x_i) \right)$
        -   因为 $\sum_i p(x_i) = 1$, 所以 $\log(\sum p(x_i)) = \log 1 = 0$。
        -   $-I(X; y_j) \le 0 \implies I(X; y_j) \ge 0$。
        -   $I(X;Y) = \sum_j p(y_j) I(X; y_j)$ 是非负项 $p(y_j) \ge 0$ 和 $I(X; y_j) \ge 0$ 的加权和,
        -   因此 $I(X;Y) \ge 0$。 证毕。

### 4.3.3 熵函数与平均互信息量之间的关系

1.  **互易性:** $I(X;Y) = I(Y;X)$

2.  **关系式 1:** $I(X;Y) = H(X) - H(X|Y)$
    * **物理意义: 平均互信息 = 先验平均不确定性 - 后验平均不确定性**
    * **证明:**
        -   $I(X;Y) = \sum \sum p(x_i y_j) I(x_i; y_j) = \sum \sum p(x_i y_j) \log \frac{p(x_i|y_j)}{p(x_i)}$
        -   $= \sum \sum p(x_i y_j) \left[ \log\frac{1}{p(x_i)} - \log\frac{1}{p(x_i|y_j)} \right]$
        -   $= \sum \sum p(x_i y_j) \log\frac{1}{p(x_i)} - \sum \sum p(x_i y_j) \log\frac{1}{p(x_i|y_j)}$
        -   第一项: $\sum_i \left(\sum_j p(x_i y_j)\right) \log\frac{1}{p(x_i)} = \sum_i p(x_i) \log\frac{1}{p(x_i)} = H(X)$
        -   第二项: $\sum \sum p(x_i y_j) \log\frac{1}{p(x_i|y_j)} = H(X|Y)$ (根据条件熵定义)
        -   $= H(X) - H(X|Y)$。 证毕。

3.  **互信息量的界:**
    * 由 $H(X|Y) \ge 0$ (不确定性非负) 和 $H(X|Y) \le H(X)$ (见 4.2.4)。
    * 可得: $0 \le I(X;Y) \le H(X)$
    * **物理意义: 信道传递的信息量不会超过信源的熵，信息量只会减少不会增加。**

4.  **关系式 2:** $I(X;Y) = H(Y) - H(Y|X)$ (证明同上)

5.  **关系图 (Venn 图):**
    * $I(X;Y)$ 是 $H(X)$ 和 $H(Y)$ 的交集。
    * $H(X) = H(X|Y) + I(X;Y)$
    * $H(Y) = H(Y|X) + I(X;Y)$
    * $H(XY) = H(X|Y) + I(X;Y) + H(Y|X)$
    * $H(XY) = H(X) + H(Y|X) = H(Y) + H(X|Y)$

6.  **其他关系式 (利用 $I(X;Y) \ge 0$):**
    * (1) $H(X|Y) \le H(X)$
        * **证明:** $I(X;Y) = H(X) - H(X|Y) \ge 0 \implies H(X) \ge H(X|Y)$。
    * (2) $H(Y|X) \le H(Y)$ (同理)
    * (3) $H(XY) \le H(X) + H(Y)$
        * **证明:**
            -   $$\begin{aligned}H(XY) &= \sum \sum p(x_i y_j) \log\frac{1}{p(x_i y_j)}\\&= \sum \sum p(x_i y_j) \log\frac{1}{p(x_i) p(y_j|x_i)}\\&= \sum \sum p(x_i y_j) \log\frac{1}{p(x_i)} + \sum \sum p(x_i y_j) \log\frac{1}{p(y_j|x_i)}\\&= \sum_i p(x_i) \log\frac{1}{p(x_i)} + H(Y|X)\\&= H(X) + H(Y|X)\end{aligned}$$
            -   利用 (2), $H(Y|X) \le H(Y)$。
            -   $\implies H(XY) \le H(X) + H(Y)$。
            -   (等号当 X, Y 统计独立时成立, 此时 $H(Y|X) = H(Y)$)

### 4.3.4 离散信道的容量

* **信息传输速率:** (单位时间传输的平均信息量)
    $R_I = I(X;Y) \times R_s$ ($R_s$ 为符号速率)
* $R_I$ 是信源 $p(x_i)$ 和信道 $p(y_j|x_i)$ 的函数。
#### 定义 4.3.3 (信道容量)
信道的**最大信息传输速率**定义为**信道容量** $C$。
$$
C = \max_{\{p(x_i)\}} R_I = \max_{\{p(x_i)\}} [I(X;Y) \times R_s]
$$

* **匹配信源:** 能使 $R_I$ 达到 $C$ (即 $I(X;Y)$ 达到最大) 的信源分布 $\{p(x_i)\}$。
* **求解匹配信源和 $I_{max}$:**
    * (使用拉格朗日极值法, 目标 $\max I(X;Y)$, 约束 $\sum p(x_i) = 1$)
    * 求解 $I(X;Y) = \sum_i \sum_j p(y_j|x_i) p(x_i) \log \frac{p(y_j|x_i)}{p(y_j)}$
    * (推导略) 匹配信源 $p_m(x_i)$ 和 $I_{max}$ 必须满足方程组：
        $$
        \begin{cases}
        \sum_{j=1}^{N} p(y_j|x_i) \log \frac{p(y_j|x_i)}{p(y_j)} = \lambda = I_{max}, & i=1, \dots, M \\
        \sum_{i=1}^{M} p(x_i) = 1
        \end{cases}
        $$
    * **求解 $I_{max}$:**
        1.  重写 $I_{max}$ 方程: $\sum_j p(y_j|x_i) [\log p(y_j|x_i) - \log p(y_j)] = I_{max}$
        2.  $\sum_j p(y_j|x_i) \log p(y_j|x_i) = \sum_j p(y_j|x_i) [I_{max} + \log p(y_j)]$  $(因为\sum_j p(y_j|x_i)=1)$
        3.  令 $\beta_j = I_{max} + \log p(y_j)$
        4.  得到关于 $\beta_j$ 的 $M$ 个线性方程 (未知数为 $N$ 个 $\beta_j$):
            $$\sum_{j=1}^{N} p(y_j|x_i) \beta_j = \sum_{j=1}^{N} p(y_j|x_i) \log p(y_j|x_i), \quad i=1, \dots, M$$
        5.  从 $\beta_j$ 的定义: $\log p(y_j) = \beta_j - I_{max} \implies p(y_j) = 2^{\beta_j - I_{max}}$
        6.  利用 $\sum p(y_j) = 1$:
            $\sum_{j=1}^{N} 2^{\beta_j - I_{max}} = 2^{-I_{max}} \sum_{j=1}^{N} 2^{\beta_j} = 1$
        7.  $2^{I_{max}} = \sum_{j=1}^{N} 2^{\beta_j}$
        8.  $I_{max} = \log \left(\sum_{j=1}^{N} 2^{\beta_j}\right)$

* **求解步骤总结:**
    1.  **解 $\beta_j$:**
        解 $M$ 个 $N$ 元线性方程组 $\sum_{j=1}^{N} p(y_j|x_i) \beta_j = \sum_{j=1}^{N} p(y_j|x_i) \log p(y_j|x_i)$ ( $i=1, \dots, M$)，获得 $\beta_j, (j=1, \dots, N)$。
    2.  **求 $I_{max}$:**
        $I_{max} = \log (\sum_{j=1}^{N} 2^{\beta_j})$
    3.  **求 $p(y_j)$:**
        $p(y_j) = 2^{\beta_j - I_{max}}$
    4.  **求 $p_m(x_i)$:** (匹配信源)
        解 $N$ 个 $M$ 元线性方程组 $p(y_j) = \sum_{i=1}^{M} p(y_j|x_i) p_m(x_i)$ ( $j=1, \dots, N$)。
    5.  **求 $C$:**
        $C = I_{max} \times R_s$
#### 【例4.3.2】
* **问题:** 转移矩阵 $P(Y|X) = \begin{bmatrix} 1/2 & 1/4 & 0 & 1/4 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 1/4 & 0 & 1/4 & 1/2 \end{bmatrix}$。 $R_s = 1000$ 波特。求匹配信源和 $C$。
* **解:**
    1.  **步骤1 (解 $\beta_j$):**
        -   $C_i = \sum_j p(y_j|x_i) \log p(y_j|x_i)$
        -   $C_1 = \frac{1}{2}\log\frac{1}{2} + \frac{1}{4}\log\frac{1}{4} + 0 + \frac{1}{4}\log\frac{1}{4} = -0.5 - 0.5 - 0.5 = -1.5$
        -   $C_2 = 1 \log 1 = 0$
        -   $C_3 = 1 \log 1 = 0$
        -   $C_4 = \frac{1}{4}\log\frac{1}{4} + 0 + \frac{1}{4}\log\frac{1}{4} + \frac{1}{2}\log\frac{1}{2} = -0.5 - 0.5 - 0.5 = -1.5$
        -   解方程组 $\sum_j p(y_j|x_i) \beta_j = C_i$:
            -   (i=1): $\frac{1}{2}\beta_1 + \frac{1}{4}\beta_2 + 0 + \frac{1}{4}\beta_4 = -1.5$
            -   (i=2): $\beta_2 = 0$
            -   (i=3): $\beta_3 = 0$
            -   (i=4): $\frac{1}{4}\beta_1 + 0 + \frac{1}{4}\beta_3 + \frac{1}{2}\beta_4 = -1.5$
        -   代入 $\beta_2=0, \beta_3=0$:
            -   $\frac{1}{2}\beta_1 + \frac{1}{4}\beta_4 = -1.5$
            -   $\frac{1}{4}\beta_1 + \frac{1}{2}\beta_4 = -1.5$
        -   解得: $\beta_1 = -2, \beta_4 = -2$。
        -   结果: $\beta_1 = -2, \beta_2 = 0, \beta_3 = 0, \beta_4 = -2$。
    2.  **步骤2 (求 $I_{max}$):**
        -   $I_{max} = \log (2^{\beta_1} + 2^{\beta_2} + 2^{\beta_3} + 2^{\beta_4})$
        -   $I_{max} = \log (2^{-2} + 2^0 + 2^0 + 2^{-2}) = \log (0.25 + 1 + 1 + 0.25) = \log(2.5)$ (或 $\log(5/2)$)
    3.  **步骤3 (求 $p(y_j)$):**
        -   $p(y_j) = 2^{\beta_j - I_{max}}$
        -   $p(y_1) = 2^{-2 - \log(5/2)} = 2^{-2} / 2^{\log(5/2)} = \frac{1/4}{5/2} = 1/10$
        -   $p(y_2) = 2^{0 - \log(5/2)} = 1 / (5/2) = 2/5$
        -   $p(y_3) = 2^{0 - \log(5/2)} = 2/5$
        -   $p(y_4) = 2^{-2 - \log(5/2)} = 1/10$
    4.  **步骤4 (求 $p_m(x_i)$):**
        -   解 $\sum_i p(y_j|x_i) p_m(x_i) = p(y_j)$:
            -   (j=1): $\frac{1}{2}p_m(x_1) + 0 + 0 + \frac{1}{4}p_m(x_4) = 1/10$
            -   (j=2): $\frac{1}{4}p_m(x_1) + 1 p_m(x_2) + 0 + 0 = 2/5$
            -   (j=3): $0 + 0 + 1 p_m(x_3) + \frac{1}{4}p_m(x_4) = 2/5$
            -   (j=4): $\frac{1}{4}p_m(x_1) + 0 + 0 + \frac{1}{2}p_m(x_4) = 1/10$
        -   解 (j=1) 和 (j=4) 得: $p_m(x_1) = 4/30, p_m(x_4) = 4/30$。
        -   代入 (j=2) 得: $p_m(x_2) = 2/5 - \frac{1}{4}(4/30) = 11/30$。
        -   代入 (j=3) 得: $p_m(x_3) = 2/5 - \frac{1}{4}(4/30) = 11/30$。
        -   匹配信源: $p_m(x_1) = 4/30, p_m(x_2) = 11/30, p_m(x_3) = 11/30, p_m(x_4) = 4/30$。
    5.  **步骤5 (求 $C$):**
        -   $C = I_{max} \times R_s = \log(2.5) \times 1000 \approx 1.32 \times 1000 = 1320$ (bit/s)
* (注意: 此方法未约束 $p_m(x_i) \ge 0$, 可能导致 $p_m(x_i) < 0$ 的不合理结果, 此时需另行处理。)

### 4.3.5 离散无记忆对称信道及特性

* **对称信道:** 转移概率矩阵中的各行元素集合相同，各列元素集合也相同。

* **性质1 (常数):**
    * $\sum_{j=1}^{N} p(y_j|x_i) \log p(y_j|x_i) = K_1$ (常数, 对所有 $i$)
    * $\sum_{i=1}^{M} p(y_j|x_i) \log p(y_j|x_i) = K_2$ (常数, 对所有 $j$)
* **性质2 (条件熵为常数):**
    * $H(Y|X)$ 与信源的统计特性 $\{p(x_i)\}$ 无关。
    * **证明:**
        -   $H(Y|X) = -\sum_i \sum_j p(x_i) p(y_j|x_i) \log p(y_j|x_i)$
        -   $= -\sum_i p(x_i) \left[ \sum_j p(y_j|x_i) \log p(y_j|x_i) \right]$
        -   根据性质1, 中括号内 $\sum_j \dots$ 是常数 $K_1$。
        -   $H(Y|X) = -\sum_i p(x_i) [K_1] = -K_1 \sum_i p(x_i) = -K_1$ (常数)
        -   $H(Y|X) = -\sum_{j=1}^{N} p(y_j|x_i) \log p(y_j|x_i)$ (对任意 $i$ )
    * 同理可证 $H(X|Y)$ 也是常数 $K_C$。
* **性质3 (等概输入 $\to$ 等概输出):**
    * 若输入 $p(x_i) = 1/M$ (等概),
    * **推导:**
        -   $p(y_j) = \sum_i p(x_i) p(y_j|x_i) = \sum_i \frac{1}{M} p(y_j|x_i)$
        -   $= \frac{1}{M} \sum_i p(y_j|x_i)$
        -   (由于对称信道 $p(y_j|x_i)$ 列元素集合相同, $\sum_i p(y_j|x_i)$ 对所有 $j$ 是一个常数, 记为 $K'$。 $\sum_j p(y_j) = 1 \implies \sum_j \frac{K'}{M} = \frac{N K'}{M} = 1 \implies K' = M/N$)
        -   $p(y_j) = \frac{1}{M} \cdot \frac{M}{N} = \frac{1}{N}$
    * 则输出 $p(y_j) = 1/N$ (等概)。

* **对称信道的容量:**
    * $C = \max_{\{p(x_i)\}} [I(X;Y) \times R_s] = \max [H(X) - H(X|Y)] \times R_s$
    * $H(X|Y)$ 是常数 $K_C$ (根据性质2)。
    * $\max H(X)$ 发生在等概输入 $p(x_i) = 1/M$ 时, $\max H(X) = \log M$。
    * 因此，对称信道的**匹配信源**是**等概信源**。
    * $$C = [\log M - K_C] \cdot R_s \quad (\text{其中 } K_C = H(X|Y) \text{ 是常数})$$
    *  **更严谨的证明:**
	    * 信道容量 $C$ 的定义为 $C = \max_{\{p(x_i)\}} [I(X;Y) \times R_s]$。
	    * 我们使用平均互信息的等价形式：$I(X;Y) = H(Y) - H(Y|X)$。
	    * $C = \max_{\{p(x_i)\}} [H(Y) - H(Y|X)] \times R_s$
	    * **步骤 1：分析 $H(Y|X)$**
	        * 根据**性质2**，对于对称信道，$H(Y|X)$ 是一个与信源统计特性 $\{p(x_i)\}$ 无关的常数。
	        * 设 $H(Y|X) = K_1$ (常数)，其中 $K_1 = -\sum_{j=1}^{N} p(y_j|x_i) \log_2 p(y_j|x_i)$ (可取转移矩阵的任意一行 $i$ 计算)。
	    * **步骤 2：分析 $\max [H(Y)]$**
	        * $C = \max_{\{p(x_i)\}} [H(Y) - K_1] \times R_s = \left( \max_{\{p(x_i)\}} [H(Y)] - K_1 \right) \times R_s$
	        * 这意味着，信道容量 $C$ 在 $H(Y)$ 达到最大值时取得。
	        * $H(Y)$ 的最大值在输出 $Y$ 服从**均匀分布**，即 $p(y_j) = 1/N$ 时达到，此时 $\max H(Y) = \log_2 N$。
	    * **步骤 3：找到匹配信源**
	        * 根据**性质3**，对于对称信道，当输入 $p(x_i) = 1/M$ (等概) 时，输出 $p(y_j) = 1/N$ (等概)。
	        * 因此，能使 $H(Y)$ 达到最大的信源分布，就是等概输入信源 $p(x_i) = 1/M$。
	        * **结论**：对称信道的匹配信源是**等概信源**。
	    * **步骤 4：计算容量 $C$**
	        * 将匹配信源 $p(x_i) = 1/M$ 代入，此时 $H(Y) = \log_2 N$，且 $H(Y|X) = K_1$。
	        * $$C = [\log_2 N - H(Y|X)] \cdot R_s$$
	        * (注：此时 $H(X) = \log_2 M$，根据熵的关系 $H(X) - H(X|Y) = H(Y) - H(Y|X)$，可以推得 $H(X|Y) = H(X) + H(Y|X) - H(Y) = \log_2 M + K_1 - \log_2 N$，这也是一个常数。因此，你原来使用的公式 $C = [\log_2 M - H(X|Y)] \cdot R_s$ 也是完全正确的，只是用 $H(Y|X)$ 推导的逻辑更直接。)

## 4.4 连续信源、信道及容量

### 4.4.1 连续信源的相对熵

* 连续信源 $X(t)$ $\to$ 连续随机变量 $X$，统计特性由概率密度函数 $p(x)$ 描述。
* **离散化 (Quantization):**
    * 将 $X$ 的取值范围 $[a_x, b_x]$ 做 $n$ 等分，每段长度 $\Delta = (b_x - a_x) / n$。
    * $X$ 取值落在第 $i$ 个区间 $[a_x + (i-1)\Delta, a_x + i\Delta]$ 的概率为：
        $$P(x_i) = \int_{a_x+(i-1)\Delta}^{a_x+i\Delta} p(x) dx$$
    * 当 $\Delta$ 足够小时, $\Delta \to 0$:
        $$P(x_i) \approx p(x_i) \Delta$$
    * 连续随机变量 $X$ 可近似为一个 $n$ 个离散值的离散随机变量 $X_I$。
        $$
        \begin{matrix}
        X_I: & x_1 & x_2 & \cdot\cdot\cdot & x_n \\
        p(X_I): & p(x_1)\Delta & p(x_2)\Delta & \cdot\cdot\cdot & p(x_n)\Delta
        \end{matrix}
        $$

* **离散化后的熵:**
    * 仿照离散信源熵的定义，近似的离散随机变量 $X_I$ 的熵为：
    $$
    H(X_I) = -\sum_{i=1}^{n} P(x_i) \log P(x_i) = -\sum_{i=1}^{n} [p(x_i)\Delta] \log [p(x_i)\Delta]
    $$
* **连续信源的熵 (取极限):**
    $$
    H(X) = \lim_{\Delta \to 0, n \to \infty} H(X_I)
    $$
    * **推导:**
        -   $H(X_I) = -\sum_{i=1}^{n} p(x_i)\Delta [\log p(x_i) + \log \Delta]$
        -   $= -\sum_{i=1}^{n} p(x_i)\Delta \log p(x_i) - \log \Delta \sum_{i=1}^{n} p(x_i)\Delta$
        -   当 $\Delta \to 0, n \to \infty$ 时:
            -   $\sum_{i=1}^{n} p(x_i)\Delta \log p(x_i) \to \int_{a}^{b} p(x) \log p(x) dx$
            -   $\sum_{i=1}^{n} p(x_i)\Delta \to \int_{a}^{b} p(x) dx = 1$
        -   $H(X) = \lim_{\Delta \to 0} \left[ -\int_{a}^{b} p(x) \log p(x) dx - \log \Delta \right]$
        -   $H(X) = -\int_{a}^{b} p(x) \log p(x) dx - \lim_{\Delta \to 0} \log \Delta$
        -   由于 $\lim_{\Delta \to 0} \log \Delta = -\infty$ (底数>1),
        -   $H(X) = -\int_{a}^{b} p(x) \log p(x) dx + \infty$
* **结论:** 连续信源的**绝对熵**为无限大。这是因为连续随机变量有无限多种可能的取值，精确表示它需要无限多的比特。
#### 定义 4.4.1 (相对熵)
连续信源 X 的**相对熵** (也称差熵, Differential Entropy) 定义为：
$$
h(X) = -\int_{a}^{b} p(x) \log p(x) dx
$$
* **注意:** 相对熵 $h(X)$ 并不具有离散熵 $H(X)$ (信息量) 的内涵，它只是一个有限的量。
#### 【例4.4.1】
* **问题:**
    1.  某连续信源在 $[-1, 1]$ 范围内均匀分布，求其相对熵。
    2.  将信号放大一倍 (变为 $[-2, 2]$ 均匀分布)，再求其相对熵。
* **解:**
    1.  **信号 1:** $p(x) = 1 / (1 - (-1)) = 1/2$。
        $h_1(X) = -\int_{-1}^{1} \frac{1}{2} \log \frac{1}{2} dx = - \left( \log \frac{1}{2} \right) \cdot \left[ \frac{1}{2} x \right]_{-1}^{1} = -(-1) \cdot \frac{1}{2} (1 - (-1)) = 1$
    2.  **信号 2:** $p(x) = 1 / (2 - (-2)) = 1/4$。
        $h_2(X) = -\int_{-2}^{2} \frac{1}{4} \log \frac{1}{4} dx = - \left( \log \frac{1}{4} \right) \cdot \left[ \frac{1}{4} x \right]_{-2}^{2} = -(-2) \cdot \frac{1}{4} (2 - (-2)) = 2$
* **分析:** 信号简单放大，不确定性 (信息特性) 并未改变，但相对熵却增加了。这说明相对熵不具有信息熵的内涵。
* **连续随机变量的关系:** (输入 $X$, 输出 $Y$)
    * 联合概率密度: $p(xy) = p(x)p(y|x)$
    * 边缘概率密度: $p(y) = \int p(xy) dx = \int p(x) p(y|x) dx$
    * 后验概率密度: $p(x|y) = \frac{p(xy)}{p(y)} = \frac{p(x)p(y|x)}{\int p(x)p(y|x) dx}$

### 4.4.2 连续信源的相对条件熵和互信息量

* **离散化 (同 4.4.1):**
    * $X \to X_I$ ( $n$ 个区间, 宽度 $\Delta$ ), $P(x_i) \approx p(x_i)\Delta$
    * $Y \to Y_J$ ( $m$ 个区间, 宽度 $\delta$ ), $P(y_j) \approx p(y_j)\delta$
* **离散化联合概率:**
    $P(x_i y_j) \approx p(x_i y_j) \Delta \delta$
* **离散化条件概率:**
    $P(x_i|y_j) = \frac{P(x_i y_j)}{P(y_j)} \approx \frac{p(x_i y_j) \Delta \delta}{p(y_j) \delta} = \frac{p(x_i y_j)}{p(y_j)} \Delta = p(x_i|y_j) \Delta$
* **离散条件熵 $H(X_n|Y_m)$:**
    * $H(X_n|Y_m) = -\sum_i \sum_j P(x_i y_j) \log P(x_i|y_j)$
    * $\approx -\sum_i \sum_j p(x_i y_j)\Delta\delta \log [p(x_i|y_j)\Delta]$
    * $= -\sum_i \sum_j p(x_i y_j)\Delta\delta \log p(x_i|y_j) - \log \Delta \sum_i \sum_j p(x_i y_j)\Delta\delta$
* **连续条件熵 (取极限 $\Delta \to 0, \delta \to 0$):**
    * $H(X|Y) = \lim_{\Delta \to 0, \delta \to 0} H(X_n|Y_m)$
    * $\approx -\int \int p(xy) \log p(x|y) dxdy - \lim_{\Delta \to 0} \log \Delta$
    * $H(X|Y) = -\int \int p(xy) \log p(x|y) dxdy + \infty$
* **结论:** 连续信源的**绝对条件熵**也为无限大。
#### 定义 4.4.2 (相对条件熵)
连续信源 X 的**相对条件熵**定义为：
$$
h(X|Y) = -\int \int p(xy) \log p(x|y) dxdy
$$
#### 连续信源的平均互信息量

* 仿照离散信源 $I(X;Y) = H(X) - H(X|Y)$。
* $I(X;Y) = \left[ h(X) - \lim_{\Delta \to 0} \log \Delta \right] - \left[ h(X|Y) - \lim_{\Delta \to 0} \log \Delta \right]$
* $I(X;Y) = h(X) - h(X|Y)$
* **关键结论:** 连续信源的**平均互信息量 $I(X;Y)$** 等于其**相对熵 $h(X)$** 与**相对条件熵 $h(X|Y)$** 之差。
* **(推导 $I(X;Y)$ 的其他形式):**
    -   $$\begin{aligned}I(X;Y) &= h(X) - h(X|Y)\\&= -\int p(x) \log p(x) dx + \int \int p(xy) \log p(x|y) dxdy\\&= -\int \int p(xy) \log p(x) dxdy + \int \int p(xy) \log p(x|y) dxdy\\&= \int \int p(xy) \left[ \log p(x|y) - \log p(x) \right] dxdy\\&= \int \int p(xy) \log \frac{p(x|y)}{p(x)} dxdy\\&= \int \int p(xy) \log \frac{p(xy)}{p(x)p(y)} dxdy(利用p(xy) = p(x|y)p(y) = p(y|x)p(x))\\&= \int \int p(xy) \log \frac{p(y|x)}{p(y)} dxdy\\&= \int p(y) \log \frac{1}{p(y)} dy - \int \int p(xy) \log \frac{1}{p(y|x)} dxdy\\&= h(Y) - h(Y|X)\end{aligned}$$
* **关系总结:**
    * $I(X;Y) = h(X) - h(X|Y)$
    * $I(X;Y) = h(Y) - h(Y|X)$
    * $I(X;Y) = h(X) + h(Y) - h(XY)$ (其中 $h(XY)$ 是联合相对熵)
* (相对熵的作用主要体现在其与平均互信息量的关系上。)

### 4.4.3 连续信源的相对熵的最大化

#### 定理 4.4.1
连续信源的相对熵函数 $h(X)$ 是信源概率密度函数 $p(x)$ 的 $\cap$形凸函数 (上凸函数)。
* (这意味着 $h(X)$ 存在最大值。)

* **最大相对熵的条件:**
    * 类似于离散情况, $h_p(X) = -\int p(x) \log p(x) dx \le -\int p(x) \log q(x) dx$。
    * 当 $p(x)$ 使 $h(X)$ 达到最大 $h_{max}(X)$ 时，对任何其他满足相同约束条件的 $q(x)$，必须成立：
        $$
        h_{max}(X) = -\int p(x) \log p(x) dx = -\int q(x) \log p(x) dx
        $$
    * (注: 原文 (4.4.38) 开始使用 $\ln$ 代替 $\log$ 进行推导, 我们这里统一使用 $\log$。)

* **不同限定条件下的最大相对熵:**

    1.  **(峰值功率受限):** 信号幅度 $x \in [a, b]$。
        * **结论:** $h(X)$ 在**均匀分布**时达到最大。
        * $p(x) = \frac{1}{b-a}$
        * **验证 (使用最大熵条件):**
            -   $h_p(X) = -\int_a^b p(x) \log p(x) dx = -\int_a^b \frac{1}{b-a} \log \frac{1}{b-a} dx = \log(b-a)$
            -   $-\int_a^b q(x) \log p(x) dx = -\int_a^b q(x) \log \frac{1}{b-a} dx = \log(b-a) \int_a^b q(x) dx = \log(b-a)$
            -   两者相等，故 $h_{max}(X) = \log(b-a)$。

    2.  **(均值受限):** $x \in (0, \infty)$ 且 $\int_0^\infty x p(x) dx = a$ (均值为 $a$)。
        * **结论:** $h(X)$ 在**指数分布**时达到最大。
        * $p(x) = \frac{1}{a} e^{-x/a}$
        * **验证 (使用 $\ln$):**
            -   $$\begin{aligned}h_p(X) &= -\int_0^\infty p(x) \ln p(x) dx \\&= -\int_0^\infty \frac{1}{a} e^{-x/a} \ln(\frac{1}{a} e^{-x/a}) dx \\&= -\int_0^\infty p(x) (\ln\frac{1}{a} - \frac{x}{a}) dx \\ &= -(-\ln a) \int p(x)dx + \frac{1}{a} \int x p(x) dx \\&= \ln a \cdot (1) + \frac{1}{a} \cdot (a) \\&= \ln a + 1\end{aligned}$$
            -   $$\begin{aligned}-\int_0^\infty q(x) \ln p(x) dx &= -\int_0^\infty q(x) (\ln\frac{1}{a} - \frac{x}{a}) dx\\&= -(-\ln a) \int q(x)dx + \frac{1}{a} \int x q(x) dx\\&= \ln a \cdot (1) + \frac{1}{a} \cdot (a) \\&= \ln a + 1\end{aligned}$$
            -   两者相等，故 $h_{max}(X)$ 对应指数分布。

    3.  **(平均功率受限):** $\int_{-\infty}^\infty x^2 p(x) dx = P$ (平均功率为 $P$, 假设均值 $m=0$, 则 $P=\sigma^2$)。
        * **结论:** $h(X)$ 在**高斯分布**时达到最大。
        * $p(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-x^2 / (2\sigma^2)}$ (其中 $\sigma^2 = P$)
        * **验证 (使用 $\ln$):**
            -   $\begin{aligned} h_p(X) &= -\int p(x) \ln p(x) dx \\&= -\int p(x) \ln\left(\frac{1}{\sqrt{2\pi}\sigma} e^{-x^2 / (2\sigma^2)}\right) dx\\&= -\int p(x) \left[ \ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{x^2}{2\sigma^2} \right] dx\\&= -\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) \int p(x)dx + \frac{1}{2\sigma^2} \int x^2 p(x) dx\\&= \ln(\sqrt{2\pi}\sigma) + \frac{1}{2\sigma^2} (\sigma^2) \\&= \ln(\sqrt{2\pi}\sigma) + \frac{1}{2} \\&= \frac{1}{2} \ln(2\pi\sigma^2) + \frac{1}{2} \ln e \\&= \frac{1}{2} \ln(2\pi e \sigma^2)\end{aligned}$
            -   $$\begin{aligned}-\int q(x) \ln p(x) dx &= -\int q(x) \left[ \ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{x^2}{2\sigma^2} \right] dx\\&= \ln(\sqrt{2\pi}\sigma) \int q(x)dx + \frac{1}{2\sigma^2} \int x^2 q(x) dx\\&= \ln(\sqrt{2\pi}\sigma) + \frac{1}{2\sigma^2} (\sigma^2) \\&= \frac{1}{2} \ln(2\pi e \sigma^2)\end{aligned}$$
            -   两者相等，故 $h_{max}(X)$ 对应高斯分布。

### 4.4.4 加性高斯噪声干扰信道的容量

* **加性噪声信道模型:** $y = x + n$
    * $y$: 接收信号, $x$: 发送信号 (已衰减), $n$: 加性噪声
* **带宽与采样:**
    * 信道带宽为 $W$ (Hz)。
    * 根据抽样定理，无失真恢复需要 $f_s \ge 2W$。
    * 单位时间内抽样值个数为 $2W$。
* **信息速率 $R_I$:** (单位时间传递的信息量)
    $R_I = I(X;Y) \times (2W)$
    $R_I = [h(Y) - h(Y|X)] \times 2W$
* **信道容量 $C$:**
    $C = \max_{p(x)} R_I = \max_{p(x)} [I(X;Y) \times 2W]$
* **加性信道特性:**
    * 假设信号 $x$ 与噪声 $n$ 统计独立。
    * **(推导 $h(Y|X)$):**
        -   $p(xy) = p(x, n=y-x) = p(x) p(n)$
        -   转移概率密度: $p(y|x) = \frac{p(xy)}{p(x)} = \frac{p(x)p(n)}{p(x)} = p(n)$ (其中 $n = y-x$)
        -   $\begin{aligned}h(Y|X) &= -\int \int p(xy) \log p(y|x) dxdy\\&= -\int \int p(x)p(n) \log p(n) dx dn\\&= \left( \int p(x) dx \right) \left( -\int p(n) \log p(n) dn \right)\\&= 1 \cdot h(N)\end{aligned}$
    * **结论:** $h(Y|X) = h(N)$。 (加性干扰信道的后验不确定性完全由噪声决定)。
* **信道容量 (加性噪声):**
    $C = \max_{p(x)} [h(Y) - h(Y|X)] \cdot 2W = \max_{p(x)} [h(Y) - h(N)] \cdot 2W$
* **加性高斯噪声 (AWGN) 信道:**
    * 噪声 $n$ 服从高斯分布，均值为 0，方差 (功率) $N = \sigma_n^2$。
    * $p(n) = \frac{1}{\sqrt{2\pi}\sigma_n} e^{-n^2 / (2\sigma_n^2)}$
    * 噪声的相对熵 (根据 4.4.3(3), 转换 $\ln$ 为 $\log$):
        $h(N) = \frac{1}{2} \log_2(2\pi e \sigma_n^2)$
* **求 $C$:**
    * $C = [\max_{p(x)} h(Y) - h(N)] \cdot 2W$
    * 为使 $C$ 最大, 需使 $h(Y)$ 最大。
    * 接收信号 $y = x+n$，其平均功率 $\sigma_y^2 = \sigma_x^2 + \sigma_n^2 = S + N$。
    * 在平均功率 $S+N$ 受限条件下， $h(Y)$ 在 $Y$ 服从高斯分布时取最大值。
    * $X$ 和 $N$ 都是高斯变量时, $Y=X+N$ 也是高斯变量。
    * 因此，**匹配信源** $p(x)$ 应为**高斯分布** (均值为 0, 功率 $S=\sigma_x^2$)。
    * 此时, $Y$ 服从均值为 0, 功率 $S+N$ 的高斯分布。
    * $\max h(Y) = \frac{1}{2} \log_2[2\pi e (\sigma_x^2 + \sigma_n^2)] = \frac{1}{2} \log_2[2\pi e (S + N)]$
* **AWGN 信道容量推导:**
    * $C = \left[ \frac{1}{2} \log(2\pi e (S+N)) - \frac{1}{2} \log(2\pi e N) \right] \cdot 2W$
    * $C = \frac{1}{2} \cdot 2W \cdot \log\left( \frac{2\pi e (S+N)}{2\pi e N} \right)$
    * $C = W \log\left( \frac{S+N}{N} \right) = W \log\left( 1 + \frac{S}{N} \right)$
#### 香农定理 (Shannon Formula)
带宽为 $W$、信号功率为 $S$、噪声功率为 $N$ (加性高斯白噪声) 的信道，其信道容量 $C$ 为：
$$
C = W \log_2\left( 1 + \frac{S}{N} \right)
$$
* $S/N$ 为信噪比 (SNR)。

* **香农定理的重要结论:**
    1.  $C$ 随 $SNR$ 增大而增大。
    2.  $C$ 随 $W$ 增大而增大。
    3.  $W$ 和 $SNR$ 可以互换：$C$ 一定时，增加 $W$ 可以换取 $SNR$ 需求的降低，反之亦然。
    4.  若 $N \to 0$ (无噪声), 则 $C \to \infty$。
    5.  **带宽 $W \to \infty$ 时的极限 (白噪声 $N = W N_0$):**
        * $C = \lim_{W\to\infty} W \log\left( 1 + \frac{S}{W N_0} \right)$
        * (利用极限 $\lim_{x\to 0} (1+x)^{1/x} = e$)
        * **推导:**
            -   $$\begin{aligned}C &= \lim_{W\to\infty} W \log_2\left[ \left( 1 + \frac{S}{W N_0} \right)^{\frac{W N_0}{S}} \right]^{\frac{S}{W N_0}}\\&= \lim_{W\to\infty} W \cdot \frac{S}{W N_0} \cdot \log_2\left[ \left( 1 + \frac{S}{W N_0} \right)^{\frac{W N_0}{S}} \right]\\&= \frac{S}{N_0} \cdot \log_2(e) \approx 1.44 \frac{S}{N_0}\end{aligned}$$
        * **结论:** 即使带宽 $W$ 无限大, 信道容量 $C$ 也不会无限大，而是趋于一个定值。

### 4.4.5 信道容量和信道带宽的归一化分析

(假设为高斯白噪声 $N = W N_0$)

1.  **归一化信道容量 (带宽利用率):**
    * $\frac{C}{W} = \log\left( 1 + \frac{S}{N} \right)$ (单位: bit/s/Hz)
    * (见图 4.4.2) 曲线划分了物理上**可实现** (下方) 和**不可实现** (上方) 的速率区域。

2.  **归一化信道带宽:**
    * $\frac{W}{C} = \left[ \log\left( 1 + \frac{S}{N} \right) \right]^{-1}$ (单位: Hz/bit/s)
    * (见图 4.4.3) 曲线划分了物理上**可实现** (上方) 和**不可实现** (下方) 的带宽区域。
![[扫描全能王 2025-10-22 21.30_1.jpg]]
3.  **香农极限 (Shannon Limit):**
    * 衡量系统性能常用**比特能量** $E_b$。
    * $E_b = S / R_b$ (传输 1 bit 所需能量)。
    * 当 $R_b = C$ (达到容量) 时, $S = E_b C$。
    * 噪声 $N = W N_0$。
    * 代入香农公式: $C = W \log\left( 1 + \frac{E_b C}{W N_0} \right)$
    * 整理得 $E_b / N_0$ (每比特能量 / 噪声功率谱密度):
        $$
        \frac{C}{W} = \log\left( 1 + \frac{E_b}{N_0} \frac{C}{W} \right)
        $$
        $$
        2^{C/W} = 1 + \frac{E_b}{N_0} \frac{C}{W}
        $$
        $$
        \frac{E_b}{N_0} = \frac{W}{C} \left( 2^{C/W} - 1 \right)
        $$
    * **带宽效率 $W/C \to \infty$ 时的极限:** (即 $C/W \to 0$, 带宽无限大, 速率固定)
        * **推导 (利用 $\lim_{x\to 0} (a^x-1)/x = \ln a$):**
            -   $\lim_{x \to 0} \frac{2^x - 1}{x} = \ln 2$
            -   $\lim_{C/W \to 0} \frac{E_b}{N_0} = \lim_{C/W \to 0} \frac{2^{C/W} - 1}{C/W}$
            -   $\frac{E_b}{N_0} = \ln 2 \approx 0.693$
        * (注: 原文 (4.4.77) 使用 $\log e$ 推导, 结果一致。 $1/(\log_2 e) = \ln 2$)
    * **极限值 (dB):**
        $$
        \frac{E_b}{N_0} \bigg|_\text{min} = \ln 2 \approx 0.693 \implies 10 \log_{10}(0.693) \approx -1.59 \text{ dB}
        $$
    * **结论 (香农极限):**
        (见图 4.4.4) 实际系统的 $E_b/N_0$ 取值**必须大于 -1.59 dB**，否则无论用多大的带宽，都无法实现无差错传输。这是信息传输速率的理论极限。 
![[扫描全能王 2025-10-22 21.30_2.jpg|300]]
---

## 4.5 信源编码的基本概念与方法

信源编码（Source Coding）是对信源消息进行变换，以提高传输效率。

**广义的信源编码**
包括采样、量化、压缩等。
* **量化 (Quantization)**：将连续取值的信号变为有限取值的数字信号。这是一个不可逆的、有损的变换。
* **压缩 (Compression)**：对量化后信号的进一步处理，以减少冗余（可以有损或无损）。

**信源编码的基本做法**
1.  **去除冗余**：使传输符号间的相关性尽可能小（例如差分脉冲编码调制 DPCM）。
2.  **等概分布**：使编码后符号近似等概分布，使每个符号携带的信息量最大。
3.  **不等长编码**：对出现概率大的符号用短码，概率小的符号用长码。

---

### 4.5.1 离散无记忆信源 (DMS)

* **信源符号集**：$S = \{S_i, i=1, 2, \dots, L\}$。
* **离散无记忆信源 (DMS)**：信源输出的符号序列 $X_k$ 彼此统计独立。
* **有记忆信源**：信源输出的符号间（如语音、图像采样）具有相关性（冗余性）。

**编码与译码模型**
* **信源分组**：将 $J$ 个信源符号分为一组 $\bar{X}_J^{(m)} = (X_1^{(m)}, \dots, X_J^{(m)})$。
* **编码**：$C(\cdot)$。将符号组 $\bar{X}_J^{(m)}$ 映射为码字 $\bar{Y}_{n_J}^{(m)}$。
* **码元集**：$\{C_i, i=1, 2, \dots, D\}$，称为 $D$ 进制编码。
* **码字**：$\bar{Y}_{n_J}^{(m)} = (Y_1^{(m)}, \dots, Y_{n_J}^{(m)})$，码字长度为 $n_J$（$n_J$ 可以是常数或变数）。
* **译码**：$C^{-1}(\cdot)$。$\bar{X}_J^{\prime(m)} = C^{-1}(\bar{Y}_{n_J}^{(m)})$。
* **译码错误**：$\bar{X}_J^{\prime(m)} \ne \bar{X}_J^{(m)}$。
* **假设**：本节研究假定信道为**无扰信道**。

**基本定义**

**定义 4.5.1 (唯一可译码 Unique Decodability)**
若信源的每个不同符号或符号序列，编码后产生不同的码字，则称该码为唯一可译码。
* 为保证唯一可译性，码字空间必须不小于信源空间：$D^{n_J} \ge L^J$。

**定义 4.5.2 (编码速率 R)**
编码表示一个信源符号所需的平均信息量（单位：比特/信源符号）。
* **等长编码**：$R = \frac{n_J}{J} \log_2 D$
* **不等长编码**：（平均码长为 $\bar{n}_J$）$R = \frac{\bar{n}_J}{J} \log_2 D$

**定义 4.5.3 (编码效率 $\eta_C$)**
信源熵 $H(S)$ 与编码速率 $R$ 的比值。
* $\eta_C = \frac{H(S)}{R}$
* 为保证编码不丢失信息，要求 $R \ge H(S)$，因此 $\eta_C \le 1$。$R$ 与 $H(S)$ 越接近，效率越高。

---

### 4.5.2 离散无记忆信源的等长编码

**概念**：将信源的每个符号或符号组，都编码成长度相等的码字。

**1. 简单的等长编码（不考虑信源统计特性）**

**(1) 单个符号编码 ($J=1$)**
* **条件**：$D^{n_1} \ge L$
* **码长**：$n_1 = \lceil \frac{\log_2 L}{\log_2 D} \rceil$
* **速率**：$R = n_1 \log_2 D$
* **效率**：$\eta_C = \frac{H(S)}{n_1 \log_2 D}$

**(2) 联合编码 ($J > 1$)**
* **条件**：$D^{n_J} \ge L^J$
* **总码长**：$n_J = \lceil J \frac{\log_2 L}{\log_2 D} \rceil$
* **平均码长**：$n_1 = \frac{n_J}{J} = \frac{1}{J} \lceil J \frac{\log_2 L}{\log_2 D} \rceil$
* **改进**：$J$ 越大，平均每个符号“浪费”的码元数越少，效率 $\eta_C$ 越高。
* **问题**：【例 4.5.1】表明，这种编码方式忽略了信源的统计特性。对于 $H(S)$ 小的信源，效率会很低。

**2. 考虑信源统计特性的等长编码（信源划分）**

* **思路**：对 $J$ 个符号进行联合编码。当 $J$ 足够大时，长度为 $J$ 的信源符号组 $\bar{X}_J$ 的实际信息量 $I_J(\bar{X}_J)$ 将趋近于其平均信息量 $J \cdot H(S)$。
* **编码**：我们只需要对那些“高概率”的序列进行编码。
* **平均信息量**：$\bar{I}_J = \frac{I(\bar{X}_J)}{J} = \frac{-\log_2 P(\bar{X}_J)}{J}$
* **渐近等同分割特性 (AEP)**：当 $J \to \infty$ 时，$\bar{I}_J \to H(S)$（依概率收敛）。

**定义 4.5.4 (典型序列集 Typical Set)**
满足 $H(S) - \epsilon \le \frac{I(\bar{X}_J)}{J} \le H(S) + \epsilon$ 的序列 $\bar{X}_J$ 组成的集合，记为 $T_S(J, \epsilon)$。
* 等价定义：$2^{-J(H(S)+\epsilon)} \le P(\bar{X}_J) \le 2^{-J(H(S)-\epsilon)}$
* **非典型序列集**：$\overline{T_S(J, \epsilon)}$

**定理 4.5.1 (信源划分定理 / AEP)**
给定 $\epsilon > 0$，当 $J \to \infty$ 时，典型序列集的总概率 $P[T_S(J, \epsilon)] \to 1$。

-   证明 (推导过程)：
    -   令 $I(X_i)$ 为单个符号的信息量，其均值为 $E[I(X)] = H(S)$，方差为 $\sigma_I^2$。
    -   序列的平均信息量 $\bar{I}_J = \frac{1}{J} \sum_{j=1}^J I(X_j)$ 是 $J$ 个独立同分布随机变量的均值。
    -   根据弱大数定理，对于任意 $\epsilon > 0$：
        $$P(|\bar{I}_J - H(S)| > \epsilon) < \frac{\sigma_I^2}{J \epsilon^2}$$
    -   当 $J \to \infty$ 时，$\frac{\sigma_I^2}{J \epsilon^2} \to 0$。
    -   因此 $P(|\bar{I}_J - H(S)| \le \epsilon) \ge 1 - \frac{\sigma_I^2}{J \epsilon^2} \to 1$。
    -   这就是 $P[T_S(J, \epsilon)] \to 1$。

**AEP 的推论 (Corollaries)**

* **系 1 (典型序列的概率)**：若 $\bar{X}_J \in T_S(J, \epsilon)$，则 $P(\bar{X}_J) \approx 2^{-JH(S)}$。所有典型序列近似等概率。
* **系 2 (典型序列的数目)**：$|T_S(J, \epsilon)| \approx 2^{JH(S)}$。
-   证明 (推导过程)：
    -   **上界**：$1 = \sum P(\bar{X}_J) \ge \sum_{\bar{X}_J \in T_S} P(\bar{X}_J) \ge |T_S(J, \epsilon)| \cdot 2^{-J(H(S)+\epsilon)}$
        $\implies |T_S(J, \epsilon)| \le 2^{J(H(S)+\epsilon)}$
    -   **下界**：由定理 4.5.1， $P(T_S) \ge 1-\delta$。
        $1-\delta \le \sum_{\bar{X}_J \in T_S} P(\bar{X}_J) \le |T_S(J, \epsilon)| \cdot 2^{-J(H(S)-\epsilon)}$
        $\implies |T_S(J, \epsilon)| \ge (1-\delta) 2^{J(H(S)-\epsilon)}$
    -   当 $J$ 足够大时，$\delta$ 任意小，故 $|T_S(J, \epsilon)| \approx 2^{JH(S)}$。
![[扫描全能王 2025-10-22 21.30_3 1.jpg]]
**AEP 的重要结论**
1.  **高概率集**：典型序列集 $T_S$ 的总概率 $\approx 1$。
2.  **小体积极**：典型序列的个数 $|T_S| \approx 2^{JH(S)}$，远小于总序列数 $L^J = 2^{J \log_2 L}$ (因为 $H(S) \le \log_2 L$)。
3.  **编码策略**：在允许微小失真（即译码错误）的场合，我们**只需对 $T_S$ 中的典型序列进行编码**。
    * 将所有 $\approx 2^{JH(S)}$ 个典型序列一对一映射到码字。
    * 将所有非典型序列（总概率 $\to 0$）映射到同一个“错误”码字，或不编码。

**定义 4.5.5 (可达速率 Achievable Rate)**
如果存在一种编码方案，当 $J \to \infty$ 时，译码错误概率 $P_E \to 0$，则称该编码速率 $R$ 是可达的。

**定理 4.5.2 (信源编码定理)**
* 若 $R > H(S)$，则 $R$ 是可达的。
* 若 $R < H(S)$，则 $R$ 是不可达的。

-   证明 (推导过程)：
    -   **(1) $R > H(S)$**：
        -   设编码速率为 $R$，采用 $D=2$ (二进制)。
        -   编码 $J$ 个符号，码长 $n_J = J \cdot R$。
        -   可用码字总数：$2^{n_J} = 2^{JR}$。
        -   典型序列数：$|T_S(J, \epsilon)| \le 2^{J(H(S)+\epsilon)}$。
        -   因为 $R > H(S)$，我们可以选择足够小的 $\epsilon$ 使得 $R > H(S) + \epsilon$。
        -   此时，**可用码字数** $2^{JR}$ **>** **典型序列数** $2^{J(H(S)+\epsilon)}$。
        -   **编码方案**：为每个典型序列分配一个唯一码字。所有非典型序列分配同一个码字。
        -   **译码错误**：仅当信源发出了一个非典型序列时才会发生。
        -   $P_E = P(\bar{X}_J \notin T_S(J, \epsilon))$。
        -   根据 AEP (定理 4.5.1)，当 $J \to \infty$ 时，$P(\bar{X}_J \notin T_S(J, \epsilon)) \to 0$。
        -   因此 $R > H(S)$ 是可达的。
    -   **(2) $R < H(S)$**：
        -   设 $R = H(S) - 2\epsilon$。
        -   可用码字总数：$2^{JR} = 2^{J(H(S)-2\epsilon)}$。
        -   典型序列数：$|T_S(J, \epsilon)| \ge (1-\delta) 2^{J(H(S)-\epsilon)}$。
        -   此时，**可用码字数** **<<** **典型序列数**。
        -   我们最多只能为 $2^{J(H(S)-2\epsilon)}$ 个典型序列分配唯一码字。
        -   无法正确译码的典型序列的比例至少为 $1 - \frac{2^{J(H(S)-2\epsilon)}}{2^{J(H(S)-\epsilon)}} = 1 - 2^{-J\epsilon}$。（注：原文此处理解不严谨，但结论正确）。
        -   当 $J \to \infty$ 时，$P_E \to 1$。
        -   因此 $R < H(S)$ 是不可达的。

**结论**
* 【例 4.5.3】表明，要用等长编码达到高效率（如 $\eta=0.9$）和低错误率（如 $P_E=10^{-4}$），需要的 $J$ 非常大（$J \approx 10^6$），这在工程上不实用。
* 因此，不等长编码在实际中应用更广。

### 4.5.3 离散无记忆信源的不等长编码

**概念**：根据信源的统计特性，对不同信源符号（或符号组）采用不同长度的码字表示。
**基本规则**：概率大的符号 $\to$ 短码字；概率小的符号 $\to$ 长码字。

**【例 4.5.4】** $P=\{0.5, 0.25, 0.125, 0.125\}$
* **码 A (异字头码)**：$\{0, 10, 110, 111\}$
* **码 B (逗点码)**：$\{0, 01, 011, 0111\}$

**基本定义**

**定义 4.5.6 (逗点码 Comma Code)**
每个码字都含有一个特定符号（逗点）来标识码字的起点（如码 B 中的 '0'）。
* **优点**：利于同步。
* **缺点**：降低编码效率。

**定义 4.5.7 (前缀 Prefix)**
码字 $Y_1 Y_2 \dots Y_n$ 的前 $r$ 位 $Y_1 \dots Y_r$。

**定义 4.5.8 (异字头码 Prefix Code)**
码字集中任一码字都不是另一码字的字头（前缀）。
* **优点**：译码时具有**即时性**，收到一个完整码字即可译码。
* **实现**：可用**编码树**（D-ary 树）来构造，所有码字都对应树的**端节点**（叶节点）。
![[扫描全能王 2025-10-22 21.30_4.jpg|400]]
**定义 4.5.9 (平均码长 $\bar{n}$)**
码字长度 $n_l$ 的统计平均值。
* $\bar{n} = \sum_{l=1}^L n_l p(S_l)$

---

**不等长编码的基本定理**

**定理 4.5.3 (Kraft 不等式)**
一个包含 $L$ 个码字、码长为 $n_1, \dots, n_L$ 的 $D$ 元**异字头码**存在的**充要条件**是：
$$\sum_{i=1}^L D^{-n_i} \le 1$$

-   证明 (推导过程)：
    -   **必要性 (异字头码 $\implies \sum \le 1$)**：
        -   构造一个 $D$ 叉树。一个异字头码对应树上的一组端节点（叶节点）。
        -   一个深度为 $n_i$ 的码字（叶节点），对应了 $D^{-n_i}$ 的“概率”（如果树是满的且延伸到最大深度）。
        -   由于所有码字对应的子树互不重叠（异字头特性），它们所占“概率”之和不能超过根节点（总概率为 1）。
        -   因此 $\sum D^{-n_i} \le 1$。
    -   **充分性 ($\sum \le 1 \implies$ 异字头码存在)**：
        -   将码长排序 $n_1 \le n_2 \le \dots \le n_L$。
        -   在 $D$ 叉树上为 $S_1$ 分配一个深度为 $n_1$ 的节点。这“占用”了 $D^{-n_1}$ 的分支。
        -   再为 $S_2$ 分配一个深度为 $n_2$ 的可用节点，占用 $D^{-n_2}$ 的分支。
        -   以此类推，只要 $\sum D^{-n_i} \le 1$，总能为所有 $L$ 个符号找到不重叠的节点（码字）。

**定理 4.5.4 (McMillan 不等式)**
任何 $D$ 元**唯一可译码** (UD Code) 的码长 $n_1, \dots, n_L$ **必须**满足 Kraft 不等式：
$$\sum_{i=1}^L D^{-n_i} \le 1$$

-   证明 (推导过程)：
    -   考虑 $(\sum_{l=1}^L D^{-n_l})^K$，其中 $K$ 为任意正整数。
    -   展开后得到 $\sum_{l_1} \dots \sum_{l_K} D^{-(n_{l_1} + \dots + n_{l_K})}$。
    -   令 $M = n_{l_1} + \dots + n_{l_K}$ 为 $K$ 个码字拼接后的总长度。
    -   令 $B_M$ 为由 $K$ 个码字组成、总长度为 $M$ 的序列个数。
    -   $(\sum_{l=1}^L D^{-n_l})^K = \sum_M B_M D^{-M}$。
    -   总长度为 $M$ 的 $D$ 进制序列总共有 $D^M$ 种。
    -   因为该码是**唯一可译**的，所以 $K$ 个码字的不同组合（$B_M$ 个）必须映射到不同的 $D$ 进制序列。
    -   因此 $B_M \le D^M$。
    -   $(\sum D^{-n_l})^K \le \sum_M D^M D^{-M} = \sum_M 1$。
    -   $\sum_M 1$ 是可能的 $K$ 序列总长度的个数。设 $n_{\max}$ 为最大码长，则 $M \le K \cdot n_{\max}$。项数 $M$ 的个数 $\le K \cdot n_{\max}$。（注：原文此步证明含糊不清，但这是标准证明的思路）。
    -   $(\sum D^{-n_l})^K \le K \cdot n_{\max}$。
    -   $\sum D^{-n_l} \le (K \cdot n_{\max})^{1/K}$。
    -   当 $K \to \infty$ 时，$(K \cdot n_{\max})^{1/K} \to 1$。
    -   因此 $\sum D^{-n_l} \le 1$。

**推论**：
1.  McMillan 不等式表明，唯一可译码的码长受限于 $\sum D^{-n_i} \le 1$。
2.  Kraft 不等式表明，只要满足此条件，就一定能构造一个*异字头码*。
3.  结论：**任何唯一可译码，总能被一个具有相同码长分布的异字头码所替代**。因此，研究最优编码只需聚焦于异字头码。

---

**定理 4.5.5 (不等长编码定理 - 码长界限)**
对于任何 $D$ 元唯一可译码（或异字头码）：

1.  **下界**：平均码长 $\bar{n}$ 必须满足：
    $$\bar{n} \ge \frac{H(S)}{\log_2 D}$$
    （仅当 $P(S_i) = D^{-n_i}$ 时取等号）

2.  **上界**：**存在**一个异字头码，其平均码长 $\bar{n}$ 满足：
    $$\bar{n} < \frac{H(S)}{\log_2 D} + 1$$

-   证明 (推导过程)：
    -   **(1) 证明下界**：
        -   $H(S) - \bar{n} \log_2 D = -\sum P_i \log_2 P_i - (\sum P_i n_i) \log_2 D$
        -   $= \sum P_i \log_2(\frac{D^{-n_i}}{P_i})$
        -   利用信息不等式 $\ln x \le x-1$，可推导 $\log_2 x \le (x-1) \log_2 e$。
        -   $\le (\log_2 e) \sum P_i (\frac{D^{-n_i}}{P_i} - 1) = (\log_2 e) (\sum D^{-n_i} - \sum P_i)$
        -   $= (\log_2 e) (\sum D^{-n_i} - 1)$
        -   根据 Kraft/McMillan 不等式， $\sum D^{-n_i} \le 1$，所以 $(\sum D^{-n_i} - 1) \le 0$。
        -   因此 $H(S) - \bar{n} \log_2 D \le 0$，即 $\bar{n} \log_2 D \ge H(S)$。
    -   **(2) 证明上界 (Shannon 编码构造法)**：
        -   我们选择码长 $n_i$（整数），使其满足 $D^{-n_i} \le P(S_i) < D^{-(n_i-1)}$。
        -   **检查 Kraft**： $\sum D^{-n_i} \le \sum P(S_i) = 1$。满足条件，故存在这样的异字头码。
        -   **计算 $\bar{n}$**：
        -   从 $P(S_i) < D^{-(n_i-1)}$ 两边取 $\log_D$：
        -   $\log_D P(S_i) < -(n_i-1) \implies n_i - 1 < -\log_D P(S_i) \implies n_i < -\log_D P(S_i) + 1$
        -   $\bar{n} = \sum P_i n_i < \sum P_i (-\log_D P(S_i) + 1)$
        -   $\bar{n} < -\sum P_i \frac{\log_2 P(S_i)}{\log_2 D} + \sum P_i$
        -   $\bar{n} < \frac{1}{\log_2 D} [-\sum P_i \log_2 P(S_i)] + 1$
        -   $\bar{n} < \frac{H(S)}{\log_2 D} + 1$

**多个符号的不等长联合编码**
* 将 $J$ 个符号作为一组 $\bar{S}_J$ 进行不等长编码。
* 这等效于一个有 $L^J$ 个符号的新信源，其熵为 $H(\bar{S}_J) = J \cdot H(S)$。
* 设 $\bar{n}_1$ 为编码后平均每个**原始信源符号**的码元数。
* 根据定理 4.5.5，$\bar{n}_1 = \frac{\bar{n}(\bar{S}_J)}{J}$。
* 将 $H(\bar{S}_J)$ 和 $J$ 代入码长界限：
    $$\frac{H(\bar{S}_J)}{J \log_2 D} \le \bar{n}_1 < \frac{H(\bar{S}_J)}{J \log_2 D} + \frac{1}{J}$$
* 代入 $H(\bar{S}_J) = J \cdot H(S)$：
    $$\frac{H(S)}{\log_2 D} \le \bar{n}_1 < \frac{H(S)}{\log_2 D} + \frac{1}{J}$$
* **结论**：当 $J \to \infty$ 时，$1/J \to 0$，平均码长 $\bar{n}_1$ 可以无限逼近香农下界 $H(S) / \log_2 D$。
* **编码效率** $\eta = \frac{H(S)}{R} = \frac{H(S)}{\bar{n}_1 \log_2 D}$，可以逼近 1。
* 【例 4.5.7】演示了 $J=2$ 时的效率提升 ($81.1\% \to 96.1\%$)。

### 4.5.4 霍夫曼编码 (Huffman Coding)

霍夫曼编码是一种构造**最优不等长异字头码**（即 $\bar{n}$ 最小）的方法。

**霍夫曼编码实现步骤 (D 进制)**
1.  **初始化**：将 $L$ 个信源符号 $S_i$ 按概率 $P(S_i)$ 递减排列。
2.  **补全 (Padding)**：
    * 为了使每次都能合并 $D$ 个，需要信源符号数 $L$ 满足 $L \equiv 1 \pmod{D-1}$。
    * 若不满足，则增加 $m$ 个概率为 $0$ 的“虚假符号”，直到 $L+m \equiv 1 \pmod{D-1}$。
    * (课件中的方法：若 $L - a(D-1) < D$，则增加 $m = D - [L - a(D-1)]$ 个虚假符号)。
3.  **缩减 (Reduce)**：
    * 选择**概率最小**的 $D$ 个符号。
    * 将这 $D$ 个符号合并成一个**新的（父）符号**，其概率为这 $D$ 个符号的概率之和。
    * 为这 $D$ 个符号分配码元 $C_1, C_2, \dots, C_D$（作为它们码字的最后一位）。
4.  **迭代**：
    * 将这个新符号插回列表，保持概率递减顺序。
    * 此时信源缩减为 $L-D+1$ 个符号。
    * 重复步骤 3，不断合并，直到最后只剩下一个概率为 1.0 的根符号。
5.  **回溯**：从根节点出发，沿路回溯到每个叶节点（原始符号），路径上的码元序列即为该符号的霍夫曼码。

**【例 4.5.8】** $L=6, D=3$，$P=\{0.24, 0.20, 0.18, 0.16, 0.14, 0.08\}$
1.  **补全**：$L=6, D=3$。$6 \not\equiv 1 \pmod 2$。需增加 $m=1$ 个虚假符号 $S_7'$ (P=0)。
    $S = \{S_1(0.24), S_2(0.20), S_3(0.18), S_4(0.16), S_5(0.14), S_6(0.08), S_7'(0)\}$
2.  **缩减 1 ($S \to S^{(1)}$)**：
    * 合并最小 3 个：$\{S_5(0.14), S_6(0.08), S_7'(0)\} \to S_{567}(0.22)$
    * 分配码元 (0, 1, 2)
    * 新列表 $S^{(1)} = \{S_1(0.24), S_{567}(0.22), S_2(0.20), S_3(0.18), S_4(0.16)\}$
3.  **缩减 2 ($S^{(1)} \to S^{(2)}$)**：
    * 合并最小 3 个：$\{S_2(0.20), S_3(0.18), S_4(0.16)\} \to S_{234}(0.54)$
    * 分配码元 (0, 1, 2)
    * 新列表 $S^{(2)} = \{S_{234}(0.54), S_1(0.24), S_{567}(0.22)\}$
4.  **缩减 3 (Final)**：
    * 合并最后 3 个：$\{S_{234}(0.54), S_1(0.24), S_{567}(0.22)\}$
    * 分配码元 (0, 1, 2)
5.  **回溯 (从后向前读码)**：
    * $S_{234} \to 0$
    * $S_1 \to 1$
    * $S_{567} \to 2$
    * ---
    * $S_2 \to (S_{234})0 \to 00$
    * $S_3 \to (S_{234})1 \to 01$
    * $S_4 \to (S_{234})2 \to 02$
    * ---
    * $S_5 \to (S_{567})0 \to 20$
    * $S_6 \to (S_{567})1 \to 21$
    * $S_7' \to (S_{567})2 \to 22$ (虚假码，丢弃)
* **编码结果**：$S_1(1), S_2(00), S_3(01), S_4(02), S_5(20), S_6(21)$
* **平均码长** $\bar{n} = 1(0.24) + 2(0.20) + 2(0.18) + 2(0.16) + 2(0.14) + 2(0.08) = 1.76$

* **对比**：【例 4.5.9】显示，如果不加虚假符号，会导致编码非最优（$\bar{n} = 2.0$）。补全步骤是必要的。

---

**码长均匀性与方差**

**定义 4.5.13 (码长方差)**
* $\sigma_C^2 = E[(n_l - \bar{n})^2] = \sum_{l=1}^L P(S_l) (n_l - \bar{n})^2$
* 方差越小，码长越均匀，有利于缓冲和匹配恒定速率信道。

**【例 4.5.10】** $D=2$, $P=\{0.4, 0.2, 0.2, 0.1, 0.1\}$
* **方法 A**：在排序时，将合并后的新符号概率放在**等值概率的最低位置**。
    * 结果：$\{1, 01, 000, 0010, 0011\}$
    * $\bar{n}_A = 2.2$
    * $\sigma_A^2 = 1.36$
* **方法 B**：在排序时，将合并后的新符号概率放在**等值概率的最高位置**。
    * 结果：$\{00, 10, 11, 010, 011\}$
    * $\bar{n}_B = 2.2$
    * $\sigma_B^2 = 0.16$
* **结论**：两种方法 $\bar{n}$ 相同（霍夫曼码的 $\bar{n}$ 是唯一的），但**方法 B (合并后置顶) 得到的码长方差更小**，码长更均匀。

---

**霍夫曼编码的特点**

**定理 4.5.6 (霍夫曼编码的最优性)**
霍夫曼码是**最佳的**（即 $\bar{n}$ 最小的）不等长异字头码。

-   证明 (推导过程)：
    -   采用归纳法，基于缩减信源的步骤。
    -   **(1) 基础**：最后一次缩减（第 $a$ 次）得到的信源 $\{S^{(a)}\}$ 恰好有 $D$ 个符号，概率和为 1。霍夫曼编码为它们分配 $n_i = 1$，$\bar{n}(a) = 1$。这显然是最佳的。
    -   **(2) 归纳**：假设霍夫曼码对 $k$ 次缩减信源 $\{S^{(k)}\}$ 是最佳的（$\bar{n}(k)$ 最小）。
    -   需要证明它对 $\{S^{(k-1)}\}$ 也是最佳的。
    -   $\{S^{(k-1)}\}$ 通过合并 $D$ 个最小概率符号 $S_j^{(k-1)}$ 得到 $\{S^{(k)}\}$ 中的一个符号 $S_i^{(k)}$。
    -   $P(S_i^{(k)}) = \sum_{j=1}^D P(S_j^{(k-1)})$
    -   它们码长的关系是 $n_j = n_i + 1$。
    -   平均码长的关系是：$\bar{n}(k-1) = \bar{n}(k) + \sum_{j=1}^D P(S_j^{(k-1)})$。
    -   **反证法**：假设存在一个**非霍夫曼码** $\{C'\}$ 对 $\{S^{(k-1)}\}$ 更优，即 $\bar{n}'(k-1) < \bar{n}(k-1)$。
    -   可以证明（最优码的性质）：这个 $\{C'\}$ 码必然也将 $D$ 个最小概率符号分配了最长的码字（且它们可以被构造成有共同前缀）。
    -   我们将这个 $\{C'\}$ 码也缩减，得到 $\{S^{(k)}\}$ 的一个码 $\{C'^{(k)}\}$。
    -   其平均码长 $\bar{n}'(k) = \bar{n}'(k-1) - \sum_{j=1}^D P(S_j^{(k-1)})$。
    -   比较可得：$\bar{n}'(k) < \bar{n}(k)$。
    -   这与“霍夫曼码 $\bar{n}(k)$ 对 $\{S^{(k)}\}$ 是最佳的”这一归纳假设相矛盾。
    -   因此，假设不成立，霍夫曼码 $\bar{n}(k-1)$ 对 $\{S^{(k-1)}\}$ 也是最佳的。
    -   以此类推，霍夫曼码对原始信源 $\{S\}$ 是最佳的。

**多个符号的霍夫曼联合编码**
* 可以将霍夫曼编码应用于 $J$ 个符号构成的扩展信源 $\{\bar{S}_J\}$。
* 这可以使平均每个符号的码长 $\bar{n}_1$ 更容易地接近香农下界 $H(S) / \log_2 D$。

---

## 4.6 信道编码的基本概念与方法

本节研究的信道是**有噪非理想信道**，信道的输入和输出不再一定是一一对应的关系。

### 4.6.1 离散无记忆信道的转移矩阵与后验概率矩阵

**信道模型**
* **发送码字集 $\{C\}$**：$M$ 个元素，先验概率分布为 $p(c_i)$。
    $$C: \begin{pmatrix} c_1 & c_2 & \cdots & c_M \\ p(c_1) & p(c_2) & \cdots & p(c_M) \end{pmatrix}$$
* **接收码字集 $\{R\}$**：$N$ 个元素。
    $$R: \begin{pmatrix} r_1 & r_2 & \cdots & r_N \\ p(r_1) & p(r_2) & \cdots & p(r_N) \end{pmatrix}$$
* **信道转移概率矩阵 $P(R|C)$**：描述了信道的特性。
    $$P(R|C) = \begin{bmatrix} p(r_1|c_1) & p(r_2|c_1) & \cdots & p(r_N|c_1) \\ p(r_1|c_2) & p(r_2|c_2) & \cdots & p(r_N|c_2) \\ \vdots & \vdots & \ddots & \vdots \\ p(r_1|c_M) & p(r_2|c_M) & \cdots & p(r_N|c_M) \end{bmatrix}$$
* **接收概率 $p(r_j)$**：
    $$p(r_j) = \sum_{i=1}^{M} p(c_i) p(r_j|c_i), \quad j=1, 2, \dots, N$$
* **后验概率 $p(c_i|r_j)$**：
    $$p(c_i|r_j) = \frac{p(c_i) p(r_j|c_i)}{p(r_j)} = \frac{p(c_i) p(r_j|c_i)}{\sum_{k=1}^{M} p(c_k) p(r_j|c_k)}$$
* **后验概率矩阵 $P(C|R)$**：
    $$P(C|R) = \begin{bmatrix} p(c_1|r_1) & p(c_2|r_1) & \cdots & p(c_M|r_1) \\ p(c_1|r_2) & p(c_2|r_2) & \cdots & p(c_M|r_2) \\ \vdots & \vdots & \ddots & \vdots \\ p(c_1|r_N) & p(c_2|r_N) & \cdots & p(c_M|r_N) \end{bmatrix}$$
* **结论**：已知先验概率 $p(c_i)$ 和信道转移概率 $p(r_j|c_i)$，就可以完全确定后验概率 $p(c_i|r_j)$ 和接收概率 $p(r_j)$。

**译码规则的影响**
* **【例 4.6.1】**
    * **条件**：发送 $\{C: 0, 1\}$，先验等概 $p(0)=p(1)=0.5$。接收 $\{R: 0, 1\}$。
    * **信道 1 (BSC)**：$P(R|C)_1 = \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}$。
        * 规则 1: $D(0)=0, D(1)=0$。$P_E = p(1)p(0|1) + p(1)p(1|1) = 0.5(0.2+0.8) = 0.5$。
        * 规则 2: $D(0)=0, D(1)=1$。$P_E = p(0)p(1|0) + p(1)p(0|1) = 0.5 \times 0.2 + 0.5 \times 0.2 = 0.2$。
        * 规则 3: $D(0)=1, D(1)=0$。$P_E = p(0)p(0|0) + p(1)p(1|1) = 0.5 \times 0.8 + 0.5 \times 0.8 = 0.8$。
        * 规则 4: $D(0)=1, D(1)=1$。$P_E = p(0)p(0|0) + p(0)p(1|0) = 0.5(0.8+0.2) = 0.5$。
        * 结论：规则 2 错误率最低。
    * **信道 2 (BSC)**：$P(R|C)_2 = \begin{bmatrix} 0.2 & 0.8 \\ 0.8 & 0.2 \end{bmatrix}$。
        * 规则 1: $P_E = 0.5$。
        * 规则 2: $P_E = 0.8$。
        * 规则 3: $P_E = 0.2$。
        * 规则 4: $P_E = 0.5$。
        * 结论：规则 3 错误率最低。
* **启示**：
    1.  不同的译码规则对误码率影响很大。
    2.  针对不同的信道特性，需要采用不同的译码规则才能使错误概率最小。

---

### 4.6.2 最大后验概率译码准则 (MAP)

* **目标**：寻求使平均错误概率 $P_E$ 最小的译码规则。
* **平均错误概率 $P_E$**：
    $$P_E = \sum_{j=1}^{N} p(r_j) p(\hat{c}_i \ne c_i | r_j)$$
    其中 $p(\hat{c}_i \ne c_i | r_j)$ 是给定 $r_j$ 时译码错误的条件概率。
* **平均正确概率 $P_C$**：
    $$P_C = 1 - P_E = \sum_{j=1}^{N} p(r_j) p(\hat{c}_i = c_i | r_j)$$
* **准则**：为了使 $P_C$ 最大（$P_E$ 最小），译码器应在收到 $r_j$ 时，选择具有最大后验概率的 $c_i$ 作为译码输出。
* **最大后验概率 (MAP) 译码方法**：
    收到 $r_j$ 后，译码输出 $\hat{c}$ 为：
    $$\hat{c} = D(r_j) = \arg \max_{c_i} \{ p(c_1|r_j), p(c_2|r_j), \dots, p(c_M|r_j) \}$$
    其中 $\arg \max$ 表示取可使后验概率 $p(c_i|r_j)$ 达到最大的那个码字 $c_i$。

* **【例 4.6.2】**
    * **条件**：同例 4.6.1， $p(c_1)=p(c_2)=0.5$。
    * **(1) 信道 1**：$P(R|C)_1 = \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}$。
        * 由于输入等概且信道对称，输出也等概 $p(r_1)=p(r_2)=0.5$。
        * 计算后验概率：
            $p(c_1|r_1) = \frac{p(c_1)p(r_1|c_1)}{p(r_1)} = \frac{0.5 \times 0.8}{0.5} = 0.8$
            $p(c_2|r_1) = \frac{p(c_2)p(r_1|c_2)}{p(r_1)} = \frac{0.5 \times 0.2}{0.5} = 0.2$
            $p(c_1|r_2) = \frac{p(c_1)p(r_2|c_1)}{p(r_2)} = \frac{0.5 \times 0.2}{0.5} = 0.2$
            $p(c_2|r_2) = \frac{p(c_2)p(r_2|c_2)}{p(r_2)} = \frac{0.5 \times 0.8}{0.5} = 0.8$
        * 后验概率矩阵：$P(C|R) = \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}$。
        * MAP 译码规则：
            $D(r_1) = \arg \max(p(c_1|r_1), p(c_2|r_1)) = \arg \max(0.8, 0.2) = c_1$ (即 0)。
            $D(r_2) = \arg \max(p(c_1|r_2), p(c_2|r_2)) = \arg \max(0.2, 0.8) = c_2$ (即 1)。
        * 这对应例 4.6.1 中的规则 2。
        * $P_E = 1 - P_C = 1 - \sum p(r_j) p(\hat{c}_i = c_i|r_j) = 1 - (0.5 \times 0.8 + 0.5 \times 0.8) = 0.2$。
    * **(2) 信道 2**：$P(R|C)_2 = \begin{bmatrix} 0.2 & 0.8 \\ 0.8 & 0.2 \end{bmatrix}$。
        * $p(c_1|r_1) = 0.2$, $p(c_2|r_1) = 0.8$。
        * $p(c_1|r_2) = 0.8$, $p(c_2|r_2) = 0.2$。
        * MAP 译码规则：
            $D(r_1) = \arg \max(0.2, 0.8) = c_2$ (即 1)。
            $D(r_2) = \arg \max(0.8, 0.2) = c_1$ (即 0)。
        * 这对应例 4.6.1 中的规则 3。
        * $P_E = 0.2$。
    * **结论**：MAP 译码准则在各种情况下都能得到最好的译码效果。

---

### 4.6.3 最大似然译码准则 (ML)

* **推导**：MAP 准则是最大化 $p(c_i|r_j)$。
    $$p(c_i|r_j) = \frac{p(r_j|c_i) p(c_i)}{p(r_j)}$$
* **特殊条件**：
    1.  发送码字**先验等概**，即 $p(c_i) = 1/M$ (常数)。
    2.  信道为**离散无记忆对称信道 (DMC)**。
* 在这些条件下，信道输出也将等概分布，$p(r_j) = 1/N$ (常数)。
* 此时，$p(r_j)$ 和 $p(c_i)$ 均为常数，最大化 $p(c_i|r_j)$ 等价于最大化 $p(r_j|c_i)$。
    $$\max_{c_i} p(c_i|r_j) \iff \max_{c_i} p(r_j|c_i)$$
* $p(r_j|c_i)$ 称为**似然率**。
* **最大似然 (ML) 译码准则**：
    收到 $r_j$ 后，译码输出 $\hat{c}$ 为：
    $$\hat{c} = D(r_j) = \arg \max_{c_i} \{ p(r_j|c_1), p(r_j|c_2), \dots, p(r_j|c_M) \}$$
* **结论**：在信源码字先验等概和 DMC 对称信道条件下，ML 准则与 MAP 准则完全等价。ML 准则不需计算后验概率，运算更简化。

* **【例 4.6.3】**
    * **条件**：DMC 对称信道，发送码字等概 $p(c_i)=1/3$。
    * 转移矩阵 $P(R|C) = \begin{bmatrix} 0.6 & 0.2 & 0.2 \\ 0.2 & 0.6 & 0.2 \\ 0.2 & 0.2 & 0.6 \end{bmatrix}$。(注：原文矩阵有误，根据对称性推测应为 $p(r_2|c_2)=0.6$)
    * **ML 译码**：
        * 收到 $r_1$：$\arg \max(p(r_1|c_1), p(r_1|c_2), p(r_1|c_3)) = \arg \max(0.6, 0.2, 0.2) = c_1$。
        * 收到 $r_2$：$\arg \max(p(r_2|c_1), p(r_2|c_2), p(r_2|c_3)) = \arg \max(0.2, 0.6, 0.2) = c_2$。
        * 收到 $r_3$：$\arg \max(p(r_3|c_1), p(r_3|c_2), p(r_3|c_3)) = \arg \max(0.2, 0.2, 0.6) = c_3$。
    * **译码规则**：$D(r_1)=c_1, D(r_2)=c_2, D(r_3)=c_3$。
    * **正确概率** $P_C = \sum_{i,j; i=j} p(c_i) p(r_j|c_i) = p(c_1)p(r_1|c_1) + p(c_2)p(r_2|c_2) + p(c_3)p(r_3|c_3)$。
        $P_C = \frac{1}{3}(0.6 + 0.6 + 0.6) = 0.6$。
    * **错误概率** $P_E = 1 - P_C = 0.4$。

---

### 4.6.4 费诺不等式 (Fano's Inequality)

费诺不等式描述了收到码字 $R$ 后，关于发送码字 $C$ 仍然存在的**不确定性 (疑义度) $H(C|R)$ 的上限**。

* **前提**：$M$ 个元素的发送集 $\{C\}$ 和接收集 $\{R\}$。
* **错误概率 $P_E$**：
    $$P_E = \sum_{i \ne j} p(c_i, r_j) = \sum_{j} p(r_j) p(c_i \ne c_j | r_j)$$
    (假设 $c_k = r_k$)
* **疑义度 $H(C|R)$**：
    $$H(C|R) = \sum_{j=1}^{M} p(r_j) H(C|r_j)$$
* **判决的不确定性 $H(P_E)$**：
    $$H(P_E) = -P_E \log_2 P_E - (1-P_E) \log_2 (1-P_E)$$
* **定理 4.6.1 (费诺不等式)**
    $$H(C|R) \le H(P_E) + P_E \log_2 (M-1)$$
-   **证明 (推导过程)**：
    -   首先分析 $H(C|r_j)$，即收到 $r_j$ 时对 $C$ 的不确定性。
    -   $H(C|r_j) = -\sum_{i=1}^{M} p(c_i|r_j) \log p(c_i|r_j)$
    -   将其分为“正确”项 ($c_j$) 和“错误”项 ($c_i, i \ne j$)。
    -   令 $p(e|r_j) = P(\text{error} | r_j) = \sum_{i \ne j} p(c_i|r_j)$。
    -   $p(c_j|r_j) = 1 - p(e|r_j)$。
    -   $H(C|r_j) = -p(c_j|r_j) \log p(c_j|r_j) - \sum_{i \ne j} p(c_i|r_j) \log p(c_i|r_j)$
    -   $= -(1-p(e|r_j)) \log(1-p(e|r_j)) - \sum_{i \ne j} p(c_i|r_j) \log p(c_i|r_j)$
    -   (通过一系列代数变换) ...
    -   $H(C|r_j) = H(p(e|r_j)) - p(e|r_j) \sum_{i \ne j} \frac{p(c_i|r_j)}{p(e|r_j)} \log \frac{p(c_i|r_j)}{p(e|r_j)}$
    -   令 $p_i' = \frac{p(c_i|r_j)}{p(e|r_j)}$ (for $i \ne j$)。这是一个 $M-1$ 个事件的概率分布。
    -   最后一项的和式 $\sum p_i' \log p_i'$ 是一个熵 $H(p')$ 的负值。
    -   根据最大熵定理，$H(p') \le \log_2(M-1)$。
    -   因此 $H(C|r_j) \le H(p(e|r_j)) + p(e|r_j) \log_2(M-1)$。
    -   对 $j$ 求期望 (即 $\sum p(r_j) \dots$)：
        $H(C|R) = \sum p(r_j) H(C|r_j) \le \sum p(r_j) H(p(e|r_j)) + \sum p(r_j) p(e|r_j) \log_2(M-1)$
    -   $\sum p(r_j) p(e|r_j)$ 是平均错误概率 $P_E$。
    -   $\sum p(r_j) H(p(e|r_j))$ 是 $H(p(e/R))$，这是一个条件熵。
    -   因为条件熵不大于无条件熵，$H(p(e/R)) \le H(P_E)$。
    -   $H(C|R) \le H(P_E) + P_E \log_2 (M-1)$。

* **释义**：疑义度 $H(C|R)$ 由两部分组成：
    1.  **$H(P_E)$**：对判决**正确与否**的不确定性。
    2.  **$P_E \log_2(M-1)$**：当已知判决**出错**时，导致的不确定性（上限）。$P_E$ 是出错概率，$\log_2(M-1)$ 是在 $M-1$ 个剩余码字中确定正确码字所需的最大信息量（M-1 个码字等概时）。

---

### 4.6.5 信道编码定理

**联合典型序列集**
* **定义 4.6.1 (联合典型序列)**
    * 对于长为 $N$ 的序列 $x=(x_1, \dots, x_N)$ 和 $y=(y_1, \dots, y_N)$，
    * $x$ 是**典型序列** ( $x \in T_X(N,\epsilon)$ )，若：
        $$|-\frac{1}{N} \log p(x) - H(X)| \le \epsilon$$
    * $y$ 是**典型序列** ( $y \in T_Y(N,\epsilon)$ )，若：
        $$|-\frac{1}{N} \log p(y) - H(Y)| \le \epsilon$$
    * $(x, y)$ 是**联合典型序列** ( $xy \in T_{XY}(N,\epsilon)$ )，若：
        $$|-\frac{1}{N} \log p(x, y) - H(X, Y)| \le \epsilon$$
* **联合典型序列集的性质**
    * **性质 1 (条件典型集大小)**：
        $$|T_{X/Y}(N, \epsilon)| \le 2^{N(H(X/Y) + 2\epsilon)}$$
        (其中 $T_{X/Y}$ 是给定 $y$ 时 $x$ 的典型序列集)
    -   **证明 (推导过程)**：
        -   $1 = \sum_x p(x|y) \ge \sum_{x \in T_{X/Y}} \frac{p(x, y)}{p(y)}$
        -   利用典型序列的概率边界：$p(x, y) \ge 2^{-N(H(XY) + \epsilon)}$ 且 $p(y) \le 2^{-N(H(Y) - \epsilon)}$。
        -   $1 \ge \sum_{x \in T_{X/Y}} \frac{2^{-N(H(XY) + \epsilon)}}{2^{-N(H(Y) - \epsilon)}} = |T_{X/Y}| \cdot 2^{-N(H(XY) - H(Y) + 2\epsilon)}$
        -   $1 \ge |T_{X/Y}| \cdot 2^{-N(H(X|Y) + 2\epsilon)}$ (因为 $H(XY) - H(Y) = H(X|Y)$)。
        -   移项即得证。
    * **性质 2 (独立序列的联合概率和)**：
        $$(1-\epsilon) 2^{-N(I(X;Y) + 3\epsilon)} \le \sum_{xy \in T_{XY}} p(x) p(y) \le 2^{-N(I(X;Y) - 3\epsilon)}$$
        (此性质用于证明信道编码定理，表明独立随机选择的 $x, y$ 恰好是联合典型的概率)

**信道编码定理**

* **定义 4.6.2 (信道编码速率 $R_I$)**
    * 将 $K$ 比特信息编码为 $N$ 比特的码字。
    * $R_I = K / N$。
* **归一化信道容量 $C_N$**
    * $C = \max I(X;Y) \cdot R_s$ ($R_s$ 为波特率)。
    * $C_N = \frac{C}{R_s \log M} = \max \frac{I(X;Y)}{\log M}$。
    * (本节中， $C_N$ 简称为信道容量，且 $I(X;Y)$ 被用作容量的衡量标准 $C_N = \max I(X;Y)$)。
* **定义 4.6.3 (可达速率)**
    * 若对任意 $\epsilon > 0$ 和编码速率 $R_I$，当 $N$ 足够大时，存在一种编码方式使得平均误码率 $P_E < \epsilon$，则称 $R_I$ 是可达的。
* **定理 4.6.2 (信道编码定理)**
    * 给定容量为 $C_N$ 的 DMC，若 $R_I < C_N$，则 $R_I$ 是可达的。
    * (反之，若 $R_I > C_N$，则不可达)。
-   **证明 (随机编码与典型序列译码)**：
    -   **编码**：随机选择 $2^{NR_I}$ 个长度为 $N$ 的序列 $x$ 作为合法码字（码书）。
    -   **发送**：发送消息 $m$ 对应的码字 $x_m$。
    -   **接收**：收到 $y$。
    -   **译码**：在 $2^{NR_I}$ 个码字中，寻找**唯一**的 $\hat{m}$，使得 $(x_{\hat{m}}, y)$ 是**联合典型序列** ($x_{\hat{m}} y \in T_{XY}(N, \epsilon)$)。
    -   **错误**：
        1.  $(x_m, y) \notin T_{XY}$ (发送的序列与接收序列不是联合典型的)。
        2.  存在 $m' \ne m$ 使得 $(x_{m'}, y) \in T_{XY}$ (某个错误码字与接收序列是联合典型的)。
    -   **错误概率分析** (假定发送 $m=1$)：
        -   $P_E(1) = P( (x_1, y) \notin T_{XY} \text{ or } \exists m \ne 1, (x_m, y) \in T_{XY} )$。
        -   $P_E(1) \le P((x_1, y) \notin T_{XY}) + \sum_{m=2}^{2^{NR_I}} P((x_m, y) \in T_{XY})$。
        -   **第 1 项**：$P((x_1, y) \notin T_{XY}) \to 0$ (当 $N \to \infty$)，由 AEP 性质。
        -   **第 2 项**：$P((x_m, y) \in T_{XY})$。因为 $x_m$ ($m \ne 1$) 是随机独立选择的，与 $y$ (由 $x_1$ 产生) 统计独立。
        -   $P((x_m, y) \in T_{XY}) = \sum_{ (x, y) \in T_{XY} } p(x) p(y)$。
        -   根据性质 2，$\sum_{ (x, y) \in T_{XY} } p(x) p(y) \le 2^{-N(I(X;Y) - 3\epsilon)}$。
        -   $\sum_{m=2}^{2^{NR_I}} P((x_m, y) \in T_{XY}) \le (2^{NR_I} - 1) \cdot 2^{-N(I(X;Y) - 3\epsilon)}$。
        -   $\le 2^{NR_I} \cdot 2^{-N(I(X;Y) - 3\epsilon)} = 2^{N(R_I - I(X;Y) + 3\epsilon)}$。
    -   **总错误率 $P_E$**：
        $P_E \le \epsilon + 2^{N(R_I - I(X;Y) + 3\epsilon)}$ (其中 $\epsilon \to 0$)。
    -   令 $C_N = \max I(X;Y)$。
        $$P_E \le 2^{N(R_I - C_N + 3\epsilon)}$$
    -   **结论**：只要 $R_I < C_N$，我们就可以选择足够小的 $\epsilon$ 使得 $R_I - C_N + 3\epsilon < 0$。
    -   当 $N \to \infty$ 时，$P_E \to 0$。
* **启示**：
    1.  可靠传输的基本条件是 $R_I \le C_N$。
    2.  只要满足此条件，就可以通过增加码长 $N$（同时按比例增加信息位 $K$ 以保持 $R_I$ 不变），使误码率任意低。

#### 【例4.6.4】
设有一产生二进制符号序列的信源，如不加编码，经离散无记忆对称信道后，差错概率 $p=0.1$。现进行某种码率为 $R_I=0.5$ 的纠错编码，分析编码后的效果。

**解：**
**(1)** 将每两个信息码元 $a_1 a_0$ 为一组，按如下规则编码为一个传输的码字 $c_3 c_2 c_1 c_0$：
$$
c_0 = a_0 + a_1, \quad c_1 = a_0, \quad c_2 = a_0, \quad c_3 = a_1
$$
得到的编译码输出关系如表4.6.1(a)所示。在传输过程中，可能会出错，定义如下面的译码表所示的译码关系，使码字的前三位出现不多于1位的错误时，对应禁用的码组在同一列中。这样，当只有1位错误时，我们仍然能够正确地译码。

**表4.6.1(a) 编译码表**

| 信息码组 $a_1 a_0$             | 00       | 01       | 10       | 11       |
| :------------------------- | :------- | :------- | :------- | :------- |
| **编码码组 $c_3 c_2 c_1 c_0$** | **0000** | **0111** | **1001** | **1110** |
| 禁用码组                       | 1000     | 1111     | 0001     | 0110     |
|                            | 0100     | 0011     | 1101     | 1010     |
|                            | 0010     | 0101     | 1011     | 1100     |

例如，$a_1 a_0 = 01$ 生成的码组 $c_3 c_2 c_1 c_0 = 0111$。如出现1位的错误，只可能是下列三种情况之一：1111, 0011 和 0101。在接收端，将 0111, 1111, 0011 和 0101 均译码为 $a_1 a_0 = 01$，1位的错误得以纠正。按照该编码和译码规则，码字正确译码的概率为：四位码元均正确或只（在 $c_3, c_2, c_1$ 中）出现一位错误。

$$
\begin{aligned}
Q &= (1-p)^4 + p(1-p)^3 + (1-p)p(1-p)^2 + (1-p)^2 p(1-p) \\
&= (1-p)^4 + 3p(1-p)^3 \\
&= (1-0.1)^4 + 3(0.1)(1-0.1)^3 \\
&= 0.6561 + 3(0.1)(0.729) = 0.8748
\end{aligned}
$$

码字的错误概率 $P_E$ 相应地为
$$
P_E = 1 - Q = 1 - 0.8748 = 0.1252
$$
记经过这样的编码和译码系统后，信息码组中每个信息码元的错误概率由没有纠错时的 $p$ 变为 $p_1$，若已知码字正确译码的概率为 $Q$，则与每个信息码元的错误概率的关系为
$$
Q = (1-p_1)(1-p_1) = (1-p_1)^2
$$
因而有
$$
p_1 = 1 - \sqrt{Q} = 1 - \sqrt{0.8748} \approx 0.065
$$
可见该纠错编码使得误码率由原来的 0.1 下降到 0.065。

**(2)** 将每三个信息码元 $a_2 a_1 a_0$ 为一组，按如下规则编成一个传输的码字 $c_5 c_4 c_3 c_2 c_1 z_0$：
$$
z_0 = a_1 + a_2, \quad c_1 = a_0 + a_2, \quad c_2 = a_0 + a_1, \quad c_3 = a_0, \quad c_4 = a_1, \quad c_5 = a_2
$$
码率 $R = 3/6 = 1/2$ 不变，得到的编码和译码的码表如表4.6.1(b)所示。

**表4.6.1(b) 编译码表**

| 信息码组 $a_2 a_1 a_0$ | 000 | 001 | 010 | 011 | 100 | 101 | 110 | 111 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **编码输出码字 $c_5 \dots c_0$** | **000000** | **001110** | **010101** | **011011** | **100011** | **101101** | **110110** | **111000** |
| 一位码元错误<br>造成的禁用码字 | 000001 | 001111 | 010100 | 011010 | 100010 | 101100 | 110111 | 111001 |
| | 000010 | 001100 | 010111 | 011101 | 100001 | 101111 | 110100 | 111010 |
| | 000100 | 001010 | 010001 | 011111 | 100111 | 101001 | 110010 | 111100 |
| | 001000 | 000110 | 011101 | 010011 | 101011 | 100101 | 111110 | 110000 |
| | 010000 | 011110 | 000101 | 001011 | 110011 | 111101 | 100110 | 101000 |
| | 100000 | 101110 | 110101 | 111011 | 000011 | 001101 | 010110 | 011000 |
| 由 $c_5, c_2$ 码元错误<br>造成的禁用码字 | 100100 | 101010 | 110001 | 111111 | 000111 | 001001 | 010010 | 011100 |

表中的编码码组和禁用码组共有64个，包括了6位二进制码组的所有可能组合图样。某码组任意的一位或特定的两位 $c_5 c_2$ 误码，归类到该码组相应的列。……如果译码时按列进行译码，……则所有1位或个别特定的 $c_5 c_2$ 这2位的错误都可以纠正。因而一定能够正确译码的概率为：
$$
\begin{aligned}
Q &= (1-p)^6 + \binom{6}{1} p(1-p)^5 + p^2(1-p)^4 \\
&= (1-0.1)^6 + 6(0.1)(1-0.1)^5 + (0.1)^2(1-0.1)^4 \\
&= 0.5314 + 6(0.1)(0.5905) + 0.01(0.6561) \\
&= 0.5314 + 0.3543 + 0.006561 \approx 0.8923
\end{aligned}
$$
码字的错误概率 $P_E$ 相应地为
$$
P_E = 1 - Q = 1 - 0.8923 = 0.1077
$$
记经过这样的编译码系统后，信息码元的误码率由没有纠错时的 $p$ 变为 $p_2$，若已知码组 $a_2 a_1 a_0$ 正确译码的概率为 $Q$，$p_2$ 满足
$$
Q = (1-p_2)(1-p_2)(1-p_2) = (1-p_2)^3
$$
因而有
$$
p_2 = 1 - \sqrt[3]{Q} = 1 - \sqrt[3]{0.8923} \approx 0.0373
$$
编码使得误码率由原来的 0.1 下降到 0.037。

**比较：**
比较例4.6.4 所得到的(1)和(2)的结果，可发现在反映编码效率的编码速率不变的情况下，通过增大码组长度，可以使信息码元的错误概率下降。如在上例中信息码组长度由2位变为3位，相应的码字长度由4位变为6位时，信息码元的错误概率由 0.065 下降到 0.037。这就是信道编码定理起作用的一个例子。

---

## 4.7 率失真理论

连续信源包含无限大的信息量，理论上需要无限大的信息速率才能无误传输。然而，实际信道的传输能力是有限的。因此，需要对信源信号进行“压缩”处理，如抽样和量化，这是一种有损的不可逆变换。

率失真理论 (Rate-Distortion Theory) 研究的是：在给定失真上限的前提下，信号可以被压缩多少，或者说，重构信号至少需要多少信息量。这是信号量化和数据压缩的理论基础。

### 4.7.1 平均失真度

**失真函数**
* 设信道输入为 $\{X:x_{i},i=1,2,\cdot\cdot\cdot,M\}$，输出为 $\{Y:y_{j},j=1,2,\cdot\cdot\cdot,N\}$。
* 信号传输会引入失真。我们定义一个实值的非负函数 $d(x_i, y_j) \ge 0$ 来描述输入 $x_i$ 变为输出 $y_j$ 所造成的影响，称为**失真函数**。
* 这 $M \times N$ 个失真函数可以构成一个**失真矩阵** $D$：
    $$D = \begin{bmatrix} d(x_1, y_1) & d(x_1, y_2) & \cdots & d(x_1, y_N) \\ d(x_2, y_1) & d(x_2, y_2) & \cdots & d(x_2, y_N) \\ \vdots & \vdots & \ddots & \vdots \\ d(x_M, y_1) & d(x_M, y_2) & \cdots & d(x_M, y_N) \end{bmatrix}$$

**失真函数示例**

1.  **汉明失真函数 (Hamming Distortion)**
    * 定义为：
        $$d(a_i, a_j) = \begin{cases} 0, & i=j \\ 1, & i \ne j \end{cases}$$
    * 失真矩阵的主对角线元素为 0，其余均为 1。

2.  **指数失真函数 (Exponential Distortion)**
    * 定义为 (a > 1)：
        $$d(a_i, a_j) = \begin{cases} 0, & i=j \\ a^{|i-j|}, & i \ne j \end{cases}$$

**平均失真度**
* 在实际系统中，某个 $d(x_i, y_j)$ 可能很大，但其发生的概率 $p(x_i, y_j)$ 可能很小。因此，我们使用统计平均值来度量系统失真。
* **定义 4.7.1 (平均失真度 $\overline{D}$)**：
    失真度的统计平均值定义为平均失真度。
    $$\overline{D} = \sum_{i=1}^{M} \sum_{j=1}^{N} d(x_i, y_j) p(x_i, y_j) = \sum_{i=1}^{M} \sum_{j=1}^{N} d(x_i, y_j) p(x_i) p(y_j|x_i)$$
* $\overline{D}$ 与信源统计特性 $P(X)$、信道转移概率 $P(Y|X)$ 以及失真函数 $D$ 都有关。

* **【例 4.7.1】**
    * **信源**：$X: (\begin{smallmatrix} x_1 & x_2 \\ 1/2 & 1/2 \end{smallmatrix})$
    * **信道**：$P(Y|X) = \begin{bmatrix} 3/4 & 1/4 \\ 1/8 & 7/8 \end{bmatrix}$ (注：原文此处 $1/3, 1/3$ 应为 $1/8, 7/8$ 以匹配后续计算)
    * **失真**：汉明失真 $D = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$。
    * **求解 $\overline{D}$**：
        $\overline{D} = \sum_{i=1}^{2} \sum_{j=1}^{2} d(x_i, y_j) p(x_i) p(y_j|x_i)$
        $= d(x_1, y_1)p(x_1)p(y_1|x_1) + d(x_1, y_2)p(x_1)p(y_2|x_1) + d(x_2, y_1)p(x_2)p(y_1|x_2) + d(x_2, y_2)p(x_2)p(y_2|x_2)$
        $= 0 \times \frac{1}{2} \times \frac{3}{4} + 1 \times \frac{1}{2} \times \frac{1}{4} + 1 \times \frac{1}{2} \times \frac{1}{8} + 0 \times \frac{1}{2} \times \frac{7}{8}$ (注：原文计算 $1/3$ 有误，此处修正)
        $= 0 + \frac{1}{8} + \frac{1}{16} + 0 = \frac{3}{16}$ (注：原文 $\frac{1}{24}$ 是基于 $1/3, 1/3$ 的错误矩阵得出的)

---

### 4.7.2 率失真函数

* 平均互信息量 $I(X;Y)$ 定义为：
    $$I(X;Y) = \sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i, y_j) I(x_i; y_j) = \sum_{i=1}^{M} \sum_{j=1}^{N} p(x_i) p(y_j|x_i) \log \frac{p(y_j|x_i)}{p(y_j)}$$
* 给定信源 $P(X)$， $I(X;Y)$ 是信道转移概率 $P(Y|X)$ 的函数。

* **定理 4.7.1**：给定信源 $P(X)$，$I(X;Y)$ 是信道转移概率 $P(Y|X)$ 的**凸函数** (原文为形凸)。
-   **证明**：
    -   设两个不同的信道转移概率 $p_1(y_j|x_i)$ 和 $p_2(y_j|x_i)$，对应的互信息量为 $I_1(X;Y)$ 和 $I_2(X;Y)$。
    -   定义一个新的转移概率 $p(y_j|x_i) = \alpha p_1(y_j|x_i) + (1-\alpha) p_2(y_j|x_i)$，其中 $0 \le \alpha \le 1$。其互信息量为 $I(X;Y)$。
    -   我们需要证明 $I(X;Y) \le \alpha I_1(X;Y) + (1-\alpha) I_2(X;Y)$。
    -   (推导过程原文较为繁琐，核心是利用 $p(x_i, y_j) = p(y_j|x_i)p(x_i) = q(x_i|y_j)p(y_j)$ 以及 $log(x)$ 函数的凸性，完整推导过程见教材162页)。
    -   $I(X;Y) - [\alpha I_1(X;Y) + (1-\alpha) I_2(X;Y)]$
    -   $= \dots$ (代入定义)
    -   $= \alpha \sum \sum p_1(x_i, y_j) \log \frac{q(x_i|y_j)}{q_1(x_i|y_j)} + (1-\alpha) \sum \sum p_2(x_i, y_j) \log \frac{q(x_i|y_j)}{q_2(x_i|y_j)}$
    -   利用 $log(x) \le (x-1) \log e$ 的性质（或对数函数的凸性），可以证明上式 $\le 0$。
    -   因此 $I(X;Y) \le \alpha I_1(X;Y) + (1-\alpha) I_2(X;Y)$。

* **率失真函数的定义**
    * $I(X;Y)$ 的凸函数特性保证了其存在最小值。
    * 给定一个平均失真度限定值 $D_C$，所有满足 $\overline{D} \le D_C$ 的信道转移矩阵 $P(Y|X)$ 的集合记为 $B_D$。
    * **定义 4.7.2 (率失真函数 $R(D_C)$)**：
        $$R(D_C) = \min_{P(Y|X) \in B_D} I(X;Y)$$
    * $R(D_C)$ 是保证信号恢复时平均失真度 $\overline{D} \le D_C$ 所需的**最小平均互信息量**。
    * $R(\overline{D})$ 是 $\overline{D}$ 的函数。
    * **典型特性** (见图 4.7.1)：
        * 当 $\overline{D} = 0$ (无失真) 时，$R(0) = \max I(X;Y) = H(X)$。
        * 当 $\overline{D} = D_{max}$ (最大失真) 时，$R(D_{max}) = 0$ (不传递任何信息)。
![[扫描全能王 2025-10-22 21.30_5.jpg|300]]
* **$R(\overline{D})$ 的定义域 $[D_{min}, D_{max}]$**

    1.  **最小允许失真度 $D_{min}$**
        * $D_{min}$ 是系统可能达到的最小平均失真度。
        * $D_{min} = \min_{\{P(Y|X)\}} \overline{D} = \min_{\{P(Y|X)\}} \sum_{i=1}^{M} \sum_{j=1}^{N} d(x_i, y_j) p(x_i) p(y_j|x_i)$
        -   **推导**：
            -   $D_{min} = \sum_{i=1}^{M} p(x_i) \left[ \min_{\{P(Y|X)\}} \sum_{j=1}^{N} d(x_i, y_j) p(y_j|x_i) \right]$
            -   要使 $\sum_{j=1}^{N} d(x_i, y_j) p(y_j|x_i)$ 最小，对于给定的 $x_i$，我们必须把所有概率 ($p(y_j|x_i)=1$) 都分配给那些使 $d(x_i, y_j)$ 最小的 $y_j$。
            -   $p(x_i) \min_{\{P(Y|X)\}} \sum_{j=1}^{N} d(x_i, y_j) p(y_j|x_i) = p(x_i) \min_{j} d(x_i, y_j)$
        * **结论**：
            $$D_{min} = \sum_{i=1}^{M} p(x_i) \min_{j} d(x_i, y_j)$$
        * 如果失真矩阵每行都有 0，则 $D_{min} = 0$。

    2.  **最大允许失真度 $D_{max}$**
        * $D_{max}$ 是满足 $R(\overline{D}) = 0$ 时对应的最小 $\overline{D}$ 值。
        * $R(\overline{D}) = 0 \implies I(X;Y) = 0$，这意味着 $X$ 和 $Y$ 统计独立，即 $p(y_j|x_i) = p(y_j)$。
        * 此时 $\overline{D} = \sum_{i} \sum_{j} d(x_i, y_j) p(x_i) p(y_j) = \sum_{j=1}^{N} p(y_j) \left[ \sum_{i=1}^{M} p(x_i) d(x_i, y_j) \right]$。
        * 我们要选择 $p(y_j)$ 分布来最小化这个 $\overline{D}$，以找到 $D_{max}$。
        -   **推导**：
            -   令 $A_j = \sum_{i=1}^{M} p(x_i) d(x_i, y_j)$。我们要最小化 $\sum_j p(y_j) A_j$。
            -   要使 $\overline{D}$ 最小，应把所有概率 ($p(y_j)=1$) 都分配给那些使 $A_j$ 最小的 $y_j$。
        * **结论**：
            $$D_{max} = \min_{j} \left\{ \sum_{i=1}^{M} p(x_i) d(x_i, y_j) \right\}$$
        -   **验证 $R(D_{max})=0$**：
            -   当 $p(y_j|x_i) = p(y_j)$ 时：
            -   $H(Y|X) = -\sum_{i} \sum_{j} p(x_i) p(y_j|x_i) \log p(y_j|x_i)$
            -   $= -\sum_{i} \sum_{j} p(x_i) p(y_j) \log p(y_j)$
            -   $= -\sum_{i} p(x_i) \left[ \sum_{j} p(y_j) \log p(y_j) \right]$
            -   $= (\sum_{i} p(x_i)) H(Y) = H(Y)$
            -   $I(X;Y) = H(Y) - H(Y|X) = H(Y) - H(Y) = 0$。

* **【例 4.7.2】**
    * **信源**：$X: (\begin{smallmatrix} x_1 & x_2 \\ p & 1-p \end{smallmatrix})$，$p \le 0.5$。
    * **失真**：汉明失真 $D = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$。
    * **(1) 求 $D_{min}$**：
        $D_{min} = \sum_{i=1}^{2} p(x_i) \min_{j} d(x_i, y_j)$
        $= p(x_1) \min(d(x_1, y_1), d(x_1, y_2)) + p(x_2) \min(d(x_2, y_1), d(x_2, y_2))$
        $= p \cdot \min(0, 1) + (1-p) \cdot \min(1, 0) = p \cdot 0 + (1-p) \cdot 0 = 0$。
        * 此时 $P(Y|X) = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$ (无失真信道)。
        * $R(D_{min}) = R(0) = I(X;Y) = H(X) = -p \log p - (1-p) \log(1-p)$。
    * **(2) 求 $D_{max}$**：
        $D_{max} = \min_{j} \left\{ \sum_{i=1}^{2} p(x_i) d(x_i, y_j) \right\}$
        $= \min \{ p(x_1)d(x_1, y_1) + p(x_2)d(x_2, y_1), \ p(x_1)d(x_1, y_2) + p(x_2)d(x_2, y_2) \}$
        $= \min \{ p \cdot 0 + (1-p) \cdot 1, \ p \cdot 1 + (1-p) \cdot 0 \}$
        $= \min(1-p, p)$。
        * 因为 $p \le 0.5$，所以 $1-p \ge p$。
        * $D_{max} = p$。
        * 此时 $R(D_{max}) = 0$。

---

**率失真函数 $R(D)$ 的主要性质**

* **定理 4.7.2**：$R(D)$ 是 $D$ 的 **U 形凸函数**。
    $R(\alpha D_1 + (1-\alpha) D_2) \le \alpha R(D_1) + (1-\alpha) R(D_2)$, $0 \le \alpha \le 1$。
-   **证明**：
    -   设 $p_1(y|x)$ 是 $R(D_1)$ 对应的信道， $p_2(y|x)$ 是 $R(D_2)$ 对应的信道。
    -   即 $\overline{D}_1 \le D_1$ 且 $R(D_1) = I_1(X;Y)$。
    -   $\overline{D}_2 \le D_2$ 且 $R(D_2) = I_2(X;Y)$。
    -   定义新信道 $p(y|x) = \alpha p_1(y|x) + (1-\alpha) p_2(y|x)$。
    -   其平均失真度 $\overline{D} = \sum \sum d(x_i, y_j) p(x_i) p(y_j|x_i)$
    -   $= \sum \sum d(x_i, y_j) p(x_i) [\alpha p_1(y_j|x_i) + (1-\alpha) p_2(y_j|x_i)]$
    -   $= \alpha \sum \sum d(x_i, y_j) p(x_i) p_1(y_j|x_i) + (1-\alpha) \sum \sum d(x_i, y_j) p(x_i) p_2(y_j|x_i)$
    -   $= \alpha \overline{D}_1 + (1-\alpha) \overline{D}_2$。
    -   令 $D = \alpha D_1 + (1-\alpha) D_2$。
    -   $\overline{D} \le \alpha D_1 + (1-\alpha) D_2 = D$。
    -   因此 $p(y|x)$ 属于 $D$ 的许可信道集 $B_D$。
    -   根据 $R(D)$ 的定义（取最小 $I(X;Y)$）和定理 4.7.1 ($I(X;Y)$ 的凸性)：
    -   $R(D) \le I(X;Y) \le \alpha I_1(X;Y) + (1-\alpha) I_2(X;Y)$
    -   $R(\alpha D_1 + (1-\alpha) D_2) \le \alpha R(D_1) + (1-\alpha) R(D_2)$。

* **定理 4.7.3**：$R(D)$ 是 $D$ 的**严格单调递减函数**。
-   **证明**：
    -   采用反证法。假设在 $D_{min} < D_1 < D_2 < D_{max}$ 区间内 $R(D_1) = R(D_2)$。
    -   令 $D_\alpha = (1-\alpha) D_1 + \alpha D_{max}$。
    -   选择足够小的 $\alpha > 0$，可以使 $D_1 < D_\alpha < D_2$。
    -   根据 $R(D)$ 的凸性 (定理 4.7.2)：
    -   $R(D_\alpha) \le (1-\alpha) R(D_1) + \alpha R(D_{max})$。
    -   根据 $D_{max}$ 的定义，$R(D_{max}) = 0$。
    -   $R(D_\alpha) \le (1-\alpha) R(D_1)$。
    -   因为 $R(D_1)$ 必然大于 0 (因为 $D_1 < D_{max}$)，且 $1-\alpha < 1$，所以：
    -   $R(D_\alpha) < R(D_1)$。
    -   我们找到了一个 $D_\alpha$ 满足 $D_1 < D_\alpha < D_2$，但 $R(D_\alpha) < R(D_1)$。
    -   这与 $R(D)$ 在 $[D_1, D_2]$ 区间恒等于 $R(D_1)$ 的假设矛盾。
    -   因此 $R(D_1) = R(D_2)$ 不成立， $R(D)$ 必为严格单调递减。

* **定理 4.7.4**：$R(D)$ 是 $D$ 的**连续函数**。
-   **证明**：
    -   $I(X;Y)$ 是 $p(y_j|x_i)$ 的连续函数，因为它是对数函数和加法/乘法的有限组合。
    -   $R(D) = \min_{P(Y|X) \in B_D} I(X;Y)$。
    -   当 $D_2 \to D_1$ (即 $\Delta \to 0$) 时，许可集 $B_{D_2}$ 也趋近于 $B_{D_1}$ ($B_{D_2} \to B_{D_1}$)。
    -   由于 $I(\cdot)$ 是连续函数，且其定义域 ($B_D$) 也是连续变化的，所以取最小值操作 $R(D)$ 也是连续的。
    -   $\lim_{\Delta \to 0} R(D_2) = R(D_1)$。

* **【例 4.7.3】**
    * **信源**：$2n$ 个等概符号，$p(x_i) = 1/(2n)$。
    * **信道**：(一个 $2n \times n$ 的转移矩阵)
        $p(y_j|x_i) = \begin{cases} 1, & i=j, i \le n \\ 1, & j=n, i > n \\ 0, & \text{else} \end{cases}$
        (即：前 $n$ 个符号无误传输；后 $n$ 个符号全部映射为 $y_n$)
    * **失真**：汉明失真。
    * **(1) 求 $\overline{D}$**：
        $\overline{D} = \sum_{i=1}^{2n} \sum_{j=1}^{n} d(x_i, y_j) p(x_i) p(y_j|x_i)$
        (只有当 $i \ne j$ 时 $d(\cdot) = 1$。根据信道定义，只有当 $i > n$ 且 $j=n$ 时才可能 $i \ne j$)
        $\overline{D} = \sum_{i=n+1}^{2n} d(x_i, y_n) p(x_i) p(y_n|x_i)$ (因为 $x_i \ne y_n=x_n$
        $\overline{D} = \sum_{i=n+1}^{2n} 1 \cdot \frac{1}{2n} \cdot 1$
        $= n \cdot \frac{1}{2n} = \frac{1}{2}$。
    * **(2) 求 $R(D)$**：
        * $H(X) = \log(2n)$ (等概信源)。
        * $H(Y|X) = 0$，因为给定 $x_i$， $y_j$ 是确定性的 (概率为 0 或 1)。
        * $I(X;Y) = H(Y) - H(Y|X) = H(Y)$。
        * 计算 $P(Y)$：
            $p(y_j) = \sum p(x_i) p(y_j|x_i)$
            $p(y_j) = p(x_j)p(y_j|x_j) = \frac{1}{2n} \cdot 1 = \frac{1}{2n}$ (对于 $j = 1, \dots, n-1$)
            $p(y_n) = p(x_n)p(y_n|x_n) + \sum_{i=n+1}^{2n} p(x_i) p(y_n|x_i)$
            $p(y_n) = \frac{1}{2n} \cdot 1 + \sum_{i=n+1}^{2n} \frac{1}{2n} \cdot 1 = \frac{1}{2n} + n \cdot \frac{1}{2n} = \frac{n+1}{2n}$。
        * $H(Y) = - \sum_{j=1}^{n} p(y_j) \log p(y_j)$
            $= - \sum_{j=1}^{n-1} \frac{1}{2n} \log \frac{1}{2n} - \frac{n+1}{2n} \log \frac{n+1}{2n}$
            $= - (n-1) \frac{1}{2n} \log \frac{1}{2n} - \frac{n+1}{2n} \log \frac{n+1}{2n}$
            $= (n-1) \frac{1}{2n} \log(2n) - \frac{n+1}{2n} (\log(n+1) - \log(2n))$
            $= (n-1) \frac{\log(2n)}{2n} - \frac{n+1}{2n} \log(n+1) + \frac{n+1}{2n} \log(2n)$
            $= \frac{(n-1) + (n+1)}{2n} \log(2n) - \frac{n+1}{2n} \log(n+1)$
            $= \log(2n) - \frac{n+1}{2n} \log(n+1)$。
        * **结论**：$R(\overline{D} = 1/2) = I(X;Y) = \log(2n) - \frac{n+1}{2n} \log(n+1)$。

---

**汉明失真与传输差错概率**
* 当采用汉明失真函数时 $d(x_i, y_j) = 1$ (if $i \ne j$) 且 $0$ (if $i=j$)。
* **平均失真度 $\overline{D}$**：
    $\overline{D} = \sum_{i=1}^{N} \sum_{j=1}^{N} d(x_i, y_j) p(x_i) p(y_j|x_i)$
    $= \sum_{i \ne j} 1 \cdot p(x_i) p(y_j|x_i) + \sum_{i=j} 0 \cdot p(x_i) p(y_j|x_i)$
    $= \sum_{i \ne j} p(x_i) p(y_j|x_i)$。
* **传输出错概率 $P_E$**：
    $P_E = \sum_{i \ne j} p(x_i, y_j) = \sum_{i \ne j} p(x_i) p(y_j|x_i)$。
* **结论**：
    $$\overline{D} = P_E$$
    即在汉明失真下，平均失真度等于传输出错的概率。